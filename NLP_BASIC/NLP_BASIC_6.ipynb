{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **카운트 기반의 단어 표현(Count based word Representation)**\n",
    "자연어 처리에서 텍스트를 표현하는 방법으로는 여러가지 방법이 있습니다. 우리가 앞서 배운 n-gram 또한 텍스트를 표현하는 방법 중 하나입니다. 하지만 머신 러닝 등의 알고리즘이 적용된 본격적인 자연어 처리를 위해서는 문자를 숫자로 수치화할 필요가 있습니다. 그런 측면에서 앞으로 문자를 숫자로 수치화하는 방법에 대해서 배우게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **1) 다양한 단어의 표현 방법**\n",
    "이번 4챕터의 제목은 카운트 기반의 단어 표현 방법이지만, 카운트 기반의 단어 표현 방법은 다음 챕터인 Bag of Words 챕터에서부터 배우게 됩니다. 지금은 카운트 기반의 단어 표현 방법 외에도 다양한 단어의 표현 방법에는 어떤 것이 있으며, 앞으로 이 책에서는 어떤 순서로 단어 표현 방법을 학습하게 될 것인지에 대해서 먼저 설명합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 단어의 표현 방법**\n",
    "단어의 표현 방법은 크게 국소 표현(Local representation) 방법과 분산 표현(Distributed Representation) 방법으로 나뉩니다. 국소 표현 방법은 해당 단어 그 자체만 보고, 특정값을 맵핑하여 단어를 표현하는 방법이며, 분산 표현 방법은 그 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법입니다.\n",
    "\n",
    "예를 들어 puppy(강아지), cute(귀여운), lovely(사랑스러운)라는 단어가 있을 때 각 단어에 1번, 2번, 3번 등과 같은 숫자를 맵핑(Mapping)하여 부여한다면 이는 국소 표현 방법에 해당됩니다. 반면, 분산 표현 방법의 예를 하나 들어보면 해당 단어를 표현하기 위해 주변 단어를 참고합니다. puppy(강아지)라는 단어 근처에는 주로 cute(귀여운), lovely(사랑스러운)이라는 단어가 자주 등장하므로, puppy라는 단어는 cute, lovely한 느낌이다로 단어를 정의합니다.\n",
    "\n",
    "* **국소 표현 방법은 단어의 의미, 뉘앙스를 표현할 수 없다.**\n",
    "* **분산 표현 방법은 단어의 뉘앙스를 표현할 수 있다.**\n",
    "\n",
    "또한 비슷한 의미로 국소 표현 방법(Local representation)을 이산 표현(Discrete Representation)이라고도 하며, 분산 표현(Distributed Representation)을 연속 표현(Continuous Represnetation)이라고도 합니다.\n",
    "\n",
    "다른 의견으로는, 구글의 연구원 토마스 미코로브(Tomas Mikolov)는 2016년에 한 발표에서 LSA나 LDA와 같은 방법들은 단어의 의미를 표현할 수 있다는 점에서 연속 표현(Continuous Represnetation)이지만, 엄밀히 말해서 다른 접근의 방법론을 사용하고 있는 워드투벡터(Word2vec)와 같은 분산 표현(Distributed Representation)은 아닌 것으로 분류하여 연속 표현을 분산 표현을 포괄하고 있는 더 큰 개념으로 설명하기도 했습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 단어 표현의 카테고리화**\n",
    "이 책에서는 아래와 같은 기준으로 단어 표현을 카테고리화하여 작성되었습니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/63136899-145a1080-c00f-11e9-9963-ad5a8576e171.png)\n",
    "\n",
    "이번 4챕터의 Bag of Words는 국소 표현에(Local Representation)에 속하며, 단어의 빈도수를 카운트(Count)하여 단어를 수치화하는 단어 표현 방법입니다. 이 챕터에서는 BoW와 그의 확장인 DTM(또는 TDM)에 대해서 학습하고, 이러한 빈도수 기반 단어 표현에 단어의 중요도에 따른 가중치를 줄 수 있는 TF-IDF에 대해서 학습합니다.\n",
    "\n",
    "6챕터에서는 단어의 뉘앙스를 반영하는 연속 표현(Continuous Representation)의 일종인 LSA를 토픽 모델링이라는 주제로 학습합니다.\n",
    "\n",
    "9챕터에서는 연속 표현(Continuous Representation)에 속하면서, 예측(Prediction)을 기반으로 단어의 뉘앙스를 표현하는 워드투벡터(Word2Vec)와 그의 확장인 패스트텍스트(FastText)를 학습하고, 예측과 카운트라는 두 가지 방법이 모두 사용된 글로브(Glove)에 대해서 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Bag of Words(BoW)\n",
    "\n",
    "이번 챕터에서는 DTM 행렬이 기본이 되는 개념이자, 단어 등장 순서를 고려하지 않는 빈도수 기반의 방법론인 Bag of Words에 대해서 학습합니다.\n",
    "\n",
    "## **1. Bag of Words란?**\n",
    "\n",
    "Bag of Words란 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법입니다. Bag of Words를 직역하면 단어들의 가방이라는 의미입니다. 단어들이 들어있는 가방을 상상해봅시다. 갖고있는 어떤 텍스트 문서에 있는 단어들을 가방에다가 전부 넣습니다. 그러고나서 이 가방을 흔들어 단어들을 섞습니다. 만약, 해당 문서 내에서 특정 단어가 N번 등장했다면, 이 가방에는 그 특정 단어가 N개 있게됩니다. 또한 가방을 흔들어서 단어를 섞었기 때문에 더 이상 단어의 순서는 중요하지 않습니다.\n",
    "\n",
    "BoW를 만드는 과정을 이렇게 두 가지 과정으로 생각해보겠습니다.\n",
    "\n",
    "1. 우선, 각 단어에 고유한 인덱스를 부여합니다.\n",
    "2. 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듭니다.\n",
    "\n",
    "한국어 예제를 통해서 BoW에 대해서 이해해보도록 하겠습니다. <br>\n",
    "**문서1 : 정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.**\n",
    "\n",
    "위의 문서1에 대해서 BoW를 만들어보도록 하겠습니다. 아래의 코드는 입력된 문서에 대해서 단어 집합(vocaburary)을 만들어 인덱스를 할당하고, BoW를 만드는 코드입니다. 이 코드에 이번에 입력할 입력은 문서1입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "okt = Okt()\n",
    "\n",
    "# 정규 표현식을 통해 온점을 제거하는 정제 작업입니다.  \n",
    "token = re.sub(\"(\\.)\", \"\", \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")\n",
    "\n",
    "# OKT 형태소 분석기를 통해 토큰화 작업을 수행한 뒤에, token에다가 넣습니다.  \n",
    "token = okt.morphs(token)\n",
    "\n",
    "word2index = {}\n",
    "bow = []\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index) # token을 읽으면서, word2index에 없는 (not in) 단어는 새로 추가하고, 이미 있는 단어는 넘깁니다.\n",
    "    \n",
    "        bow.insert(len(word2index) - 1, 1) # BoW 전체에 전부 기본값 1을 넣어줍니다. 단어의 개수는 최소 1개 이상이기 때문입니다.\n",
    "    else:\n",
    "        index = word2index.get(voca) # 재등장하는 단어의 인덱스를 받아옵니다.\n",
    "        bow[index] = bow[index] + 1 # 재등장한 단어는 해당하는 인덱스의 위치에 1을 더해줍니다. (단어의 개수를 세는 것입니다.)  \n",
    "\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서1에 각 단어에 대해서 인덱스를 부여한 결과는 첫번째 출력 결과입니다. 문서1의 BoW는 두번째 출력 결과입니다. 두번째 출력 결과를 보면, 물가상승률의 인덱스는 4이며, 문서1에서 물가상승률은 2번 언급되었기 때문에 인덱스 4(다섯번째 값)에 해당하는 값이 2임을 알 수 있습니다. (원한다면 한국어에서 불용어에 해당되는 조사들 또한 제거하여 더 정제된 BoW를 만들 수도 있습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
