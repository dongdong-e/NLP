{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **언어 모델(Language Model)**\n",
    "언어 모델(Languagel Model)이란 주어진 단어들로부터 다음에 등장할 단어의 확률을 예측하는 모델을 말합니다. 이번 챕터에서는 통계에 기반한 전통적인 언어 모델(Statistical Languagel Model, SLM)에 대해서 배웁니다. 통계에 기반한 언어 모델은 우리가 실제 사용하는 자연어를 근사하기에는 많은 한계가 있었고, 요즘 들어 인공 신경망이 그러한 한계를 많이 해결해주면서 통계 기반 언어 모델은 많이 사용 용도가 줄었습니다.\n",
    "\n",
    "하지만 그럼에도 여전히 통계 기반 언어 모델에서 배우게 될 n-gram은 자연어 처리 분야에서 활발하게 활용되고 있으며, 통계 기반 방법론에 대한 이해는 언어 모델에 대한 전체적인 시야를 갖는 일에 큰 도움이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1) 언어 모델(Language Model)이란?**\n",
    "언어 모델(Language Model)은 언어라는 현상을 표현하는 모델을 말합니다. 이 모델은 언어라는 현상을 표현, 다르게 말하면 자연어를 생성하는 일들을 하는데 결국 이러한 자연어 생성(Natural Language Generation) 작업을 통해서 음성 인식, 기계 번역, OCR, 검색어 자동 완성과 같은 일들을 수행합니다.\n",
    "\n",
    "좀 더 수학적인 표현을 사용하면 언어 모델은 단어 시퀀스의 확률을 예측하는 모델입니다. 언어 모델을 만드는 방법은 크게는 **통계를 이용한 방법**과 **인공 신경망을 이용한 방법**으로 구분할 수 있습니다. 최근 자연어 처리에서 언어 모델에 대한 이야기를 빼놓을 수가 없는데, 최근 핫한 딥 러닝 자연어 처리의 신기술인 GPT나 BERT가 전부 언어 모델의 개념을 사용하여 만들어졌기 때문입니다. 뒤의 딥 러닝 챕터에서 인공 신경망의 일종인 RNN을 통해 언어 모델을 만들어볼 겁니다.\n",
    "\n",
    "이번 챕터에서는 언어 모델의 직관과 전통적 언어 모델인 통계적 언어 모델을 배웁니다. 언어 모델은 줄여서 LM이라고도 부릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 언어 모델(Language Model)이 하는 일**\n",
    "언어 모델이 하는 일을 간단하게 요약하면 **문장의 확률**을 예측하는 일입니다. 그런데 완성된 문장의 확률을 예측하기 위해서는 우선 **이전 단어들이 주어졌을 때 다음 단어가 나올 확률을 예측**해야 합니다. 자연어 처리로 유명한 스탠포드 대학교에서는 언어 모델을 문법(grammar)이라고 비유하기도 합니다. 언어 모델이 단어들의 조합이 얼마나 적절한지, 또는 해당 문장이 얼마나 적합한지를 알려주는 일을 하기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 문장의 확률을 예측하는 일이 왜 필요한가?**\n",
    "\n",
    "언어 모델은 문장의 확률을 예측합니다. 이러한 예측이 왜 필요한지 예를 들어보겠습니다. 여기서 대문자 P는 확률을 의미합니다.\n",
    "\n",
    "### **a. 기계 번역(Machine Translation):**\n",
    "\n",
    "*P(나는 버스를 탔다) > P(나는 버스를 태운다)* <br>\n",
    ": 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단합니다.\n",
    "\n",
    "### **b. 오타 교정(Spell Correction)**\n",
    "\n",
    "선생님이 교실로 부리나케 <br>\n",
    "*P(달려갔다) > P(잘려갔다)* <br>\n",
    ": 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단합니다.\n",
    "\n",
    "### **c. 음성 인식(Speech Recognition)**\n",
    "\n",
    "*P(나는 메롱을 먹는다) < P(나는 메론을 먹는다)* <br>\n",
    ": 언어 모델은 두 문장을 비교하여 우측의 문장의 확률이 더 높다고 판단합니다.\n",
    "\n",
    "언어 모델은 위와 같이 확률을 통해 보다 적절한 문장을 판단합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. 언어 모델은 문장의 확률 또는 단어 등장 확률을 예측하는 일.**\n",
    "\n",
    "언어 모델은 문장 전체의 확률을 예측하는 일을 하며, 또한 이 일을 하기 위해서 이전 단어들이 주어졌을 때, 다음 단어가 등장할 확률을 예측하는 일도 합니다. 언어 모델이 이 두 가지 확률을 어떻게 예측하며 또한 결국 왜 이 두 가지가 연관되는 문제인지 알아보겠습니다.\n",
    "\n",
    "### **A. 문장의 확률 예측**\n",
    "하나의 단어(word)를 w라고 합시다. 단어의 시퀀스인 전체 문장(sentence)을 대문자 W라고 합시다. n개의 w로 구성된 문장 W의 확률은 다음과 같이 표현할 수 있습니다.\n",
    "\n",
    "* **P(W)=P(w1,w2,w3,w4,w5,...,wn)=∏n=1nP(wn)**\n",
    "\n",
    "### **B. 다음 단어 예측**\n",
    "n-1개의 단어가 나열된 상태에서 n번째 단어의 확률을 다음과 같이 표현할 수 있습니다.\n",
    "\n",
    "* **P(wn|w1,...,wn−1)P(wn|w1,...,wn−1)**\n",
    "\n",
    "|의 기호는 조건부 확률(conditional probability)을 의미합니다. \n",
    "예를 들어 다섯번째 단어의 확률은 아래와 같습니다.\n",
    "\n",
    "* **P(w5|w1,w2,w3,w4)P(w5|w1,w2,w3,w4)**\n",
    "\n",
    "문장. 즉, 전체 단어 시퀀스의 확률은 모든 단어가 예측되고 나서야 알 수 있으므로 결국 전체 단어 시퀀스의 예측은 다음과 같습니다.\n",
    "\n",
    "* **P(W)=P(w1,w2,w3,w4,w5,...wn)=∏n=1nP(wn|w1,...,wn−1)**\n",
    "\n",
    "혹시 조건부 확률의 개념을 모른다면, 여기서는 자세히 설명하지 않으므로 인터넷 검색 또는 이 챕터의 부록을 참고합시다.\n",
    "\n",
    "* **조건부 확률 영상**\n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/awOOws2fInc/0.jpg)](https://www.youtube.com/watch?v=awOOws2fInc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. 언어 모델의 간단한 직관**\n",
    "갑자기 수식이 나와서 언어 모델이 괜히 어려워보입니다. 하지만 직관 자체는 전혀 어렵지 않습니다. 가령 **비행기를 타려고 공항에 갔는데 지각을 하는 바람에 비행기를 [?]**라는 문장이 있습니다. **'비행기를'** 다음에 어떤 단어가 오게 될지 우리는 손쉽게 **'놓쳤다'**라고 예상할 수 있습니다. 우리는 우리의 지식에 기반하여 나올 수 있는 여러 단어들을 비교해본 결과 놓쳤다는 단어가 나올 확률이 가장 높다고 판단하였기 때문입니다.\n",
    "\n",
    "그렇다면 컴퓨터. 즉, 기계에게 위의 문장을 주고 나서, **'비행기를'** 다음에 나올 단어를 예측해보라고 한다면 과연 어떻게 최대한 정확히 예측할 수 있을까요? 기계도 비슷합니다. 앞에 어떤 단어들이 나왔는지 고려하여 후보가 될 수 있는 여러 단어들에 대해서 확률을 예측해보고 가장 높은 확률을 가진 단어를 선택합니다. 그리고 그때 사용되는 것이 바로 언어 모델입니다. 앞에 어떤 단어들이 나왔는지 고려하여 후보가 될 수 있는 여러 단어들에 대해서 확률을 예측해보고 가장 높은 확률을 가진 단어를 선택합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. 검색 엔진에서의 언어 모델의 예**\n",
    "\n",
    "![img](https://wikidocs.net/images/page/21668/%EB%94%A5_%EB%9F%AC%EB%8B%9D%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C.PNG)\n",
    "\n",
    "검색엔진은 언어 모델을 이용하여 입력된 단어들의 나열에 대해서 다음 단어를 예측합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **2) 통계적 언어 모델(Statistical Language Model, SLM)**\n",
    "\n",
    "앞 챕터에서 언어 모델(Language Model)의 개념에 대해서 간략히 설명해보았습니다. 여기서는 언어 모델의 전통적인 접근 방법인 통계적 언어 모델을 소개합니다. 통계적 언어 모델이 통계적인 접근 방법으로 어떻게 언어를 모델링 하는지 배워보겠습니다. 통계적 언어 모델(Statistical Language Model)은 줄여서 SLM이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 조건부 확률**\n",
    "\n",
    "조건부 확률은 두 확률 P(A),P(B)P(A),P(B)에 대해서 아래와 같은 관계를 갖습니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/62987298-0aeb6f80-be7a-11e9-936c-d4f0f4ed15b2.png)\n",
    "\n",
    "더 많은 확률에 대해서 일반화해봅시다. 4개의 확률이 조건부 확률의 관계를 가질 때, 아래와 같이 표현할 수 있습니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/62987319-235b8a00-be7a-11e9-8f77-0f0f4bee9964.png)\n",
    "\n",
    "이를 조건부 확률의 연쇄 법칙(chain rule)이라고 합니다. 이제는 4개가 아닌 nn개에 대해서 일반화를 해봅시다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/62987484-fbb8f180-be7a-11e9-9fe2-a5444ec61a34.png)\n",
    "\n",
    "조건부 확률에 대한 정의를 통해 문장의 확률을 구해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 실제 문장을 통한 확률식 이해**\n",
    "\n",
    "An adorable little boy is spreading smiles이라는 문장 하나에 대한 전체 확률식을 구해보겠습니다. 다시 말해 **P(An adorable little boy is spreading smiles)를 계산합니다.**\n",
    "\n",
    "각 단어는 문맥이라는 관계로 인해 이전 단어의 영향을 받아 나온 단어입니다. 그리고 모든 단어로부터 하나의 전체 문장이 완성됩니다. 그렇기 때문에 문장 전체의 확률을 구하고자 조건부 확률을 사용합니다. 앞서 언급한 조건부 확률의 일반화 식을 문장의 확률 관점에서 다시 적어보면 전체 문장의 확률은 각 단어들이 이전 단어가 주어졌을 때 다음 단어로 등장할 확률의 곱으로 구성됩니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/62987349-4ede7480-be7a-11e9-9000-79672964d587.png)\n",
    "\n",
    "위의 문장에 해당 식을 적용해보면 다음과 같습니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/62987510-168b6600-be7b-11e9-8aa4-bed48cc0f764.png)\n",
    "\n",
    "전체 문장의 확률을 계산하기 위해서 각 단어에 대한 예측 확률들을 곱합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. 카운트 기반의 접근**\n",
    "\n",
    "전체 문장의 확률을 계산하기 위해서 다음 단어에 대한 예측 확률을 모두 곱하여 구한한다는 것은 알았습니다. 그렇다면 SLM에서 실제 기계는 이전 단어로부터 다음 단어에 대한 확률은 어떻게 계산할까요? 정답은 카운트에 기반하여 확률을 계산합니다.\n",
    "\n",
    "An adorable little boy가 나왔을 때, is가 나올 확률인 P(is|An adorable little boy)계산한다고 해봅시다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/62987384-7cc3b900-be7a-11e9-8899-486577db26bd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. 카운트 기반 접근의 한계**\n",
    "\n",
    "언어 모델이 하는 일에 대해서 다시 재정의 해보자면, 실생활에서 사용되는 말의 정확한 확률 분포를 근사 모델링하는 것이 언어 모델이입니다. 실제로 정확하게 알아볼 방법은 없겠지만 현실에서도 An adorable little boy가 나왔을 때 is가 나올 확률이라는 것이 존재합니다. 이를 실제 자연어의 확률 분포, 현실에서의 확률 분포라고 명칭해볼 수 있습니다. 그리고 기계에게 많은 코퍼스를 훈련시켜서 언어 모델을 통해 현실에서의 확률 분포를 근사하는 것이 언어 모델의 목표입니다. 그런데 카운트 기반으로 접근하려고 한다면 갖고있는 코퍼스. 즉, 다시 말해 기계가 훈련하는 데이터는 방대한 양이 필요합니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/62987406-9cf37800-be7a-11e9-8e56-0bfcfb81700c.png)\n",
    "\n",
    "가령, An adorable little boy가 나왔을 때 is가 나올 확률을 계산하고 싶었지만, 기계가 훈련한 코퍼스에 An adorable little boy is라는 문장 자체가 없었다고 한다면 이 문장에 대한 확률을 계산할 수 없습니다. 단어 분리(subword segmentation) 챕터에서 배웠던 OOV 문제입니다. 코퍼스에 해당 데이터가 없다는 것은 카운트 계산 시 분자 또는 분모가 0이 된다는 뜻입니다. 그렇다면 이 확률을 0 또는 정의되지 않는 확률이라고 할 수 있을까요? 아닙니다. 현실에선 An adorable little boy is 라는 단어의 나열이 분명히 존재하고 있기 때문입니다.\n",
    "\n",
    "위의 문제를 완전히 해결할 수는 없지만, 그래도 완화하는 방법들이 있습니다. 바로 n-gram 언어 모델과 여러가지 SLM의 일반화(generalization) 기법입니다. 미리 언급하면 n-gram 언어 모델과 여러 일반화 기법들도 딥 러닝을 이용한 언어 모델에 비해서는 결국 성능이 많이 뒤떨어집니다. 하지만 n-gram도 여전히 자연어 처리에서 중요한 개념인만큼, n-gram 언어 모델에 대해서 배워보고 딥 러닝을 이용한 언어 모델은 뒤의 딥 러닝 챕터에서 배워보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **3) N-gram Language Model**\n",
    "\n",
    "n-gram 언어 모델은 여전히 카운트에 기반한 통계적 접근을 사용하고 있으므로 SLM의 일종입니다. 다만, 앞서 배운 언어 모델과는 달리 이전에 등장한 모든 단어를 고려하는 것이 아니라 일부 단어만 고려하는 접근 방법을 사용합니다. 그리고 이때 일부 단어를 몇 개 보느냐를 결정해야하는데 이것이 n-gram에서의 n이 가지는 의미입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 코퍼스에서 카운트하지 못하는 경우의 감소.**\n",
    "\n",
    "SLM의 한계는 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다는 점입니다. 그리고 확률을 계산하고 싶은 문장이 길어질수록 갖고있는 코퍼스에서 그 문장이 존재하지 않을 가능성이 높습니다. 다시 말하면 카운트할 수 없을 가능성이 높습니다. 그런데 마르코프의 가정을 사용하면 카운트를 할 수 있을 가능성이 높일 수 있습니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/62988974-3b82d780-be81-11e9-8999-9e03c972ec9b.png)\n",
    "\n",
    "가령, An adorable little boy가 나왔을 때 is가 나올 확률을 그냥 boy가 나왔을 때 is가 나올 확률로 생각해보는 건 어떨까요? 갖고있는 코퍼스에 An adorable little boy is가 있을 가능성 보다는 boy is라는 더 짧은 워드 시퀀스(word sequence)가 코퍼스에 있을 가능성이 더 높습니다.\n",
    "\n",
    "조금 지나친 일반화로 느껴진다면 아래와 같이 little boy가 나왔을 때 is가 나올 확률로 생각하는 것도 대안입니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/62989170-f27f5300-be81-11e9-9384-9591b4c78739.png)\n",
    "\n",
    "즉, 앞에서는 An adorable little boy가 나왔을 때 is가 나올 확률을 구하기 위해서는 An adorable little boy가 나온 횟수와 An adorable little boy is가 나온 횟수를 카운트해야만 했지만, 이제는 단어의 확률을 구하고자 기준 단어의 앞 단어를 전부 포함해서 세지말고, 앞 단어 중 임의의 개수만 포함해서 세보자는 것입니다. 이렇게 하면 갖고있는 코퍼스에서도 해당 단어의 나열을 카운트할 확률이 높아집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. N-gram**\n",
    "\n",
    "이 때 임의의 개수를 정하기 위한 기준을 위해 사용하는 것이 n-gram입니다. n-gram은 n개의 연속적인 단어 나열을 의미합니다. 갖고 있는 코퍼스에서 n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주합니다. 예를 들어서 문장 An adorable little boy is spreading smiles이 있을 때, 각 n에 대해서 n-gram을 전부 구해보면 다음과 같습니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/62989019-6705c200-be81-11e9-85cc-5dfaf4c1fc1d.png)\n",
    "\n",
    "n-gram을 사용할 때는 n이 1일 때는 유니그램(unigram), 2일 때는 바이그램(bigram), 3일 때는 트라이그램(trigram)이라고 명명하고 n이 4 이상일 때는 gram 앞에 그대로 숫자를 붙여서 명명합니다. 출처에 따라서는 유니그램, 바이그램, 트라이그램 또한 각각 1-gram, 2-gram, 3-gram이라고 하기도 합니다. 이제 n-gram을 이용한 언어 모델을 설계해보도록 하겠습니다.\n",
    "\n",
    "n-gram을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존합니다. 예를 들어 **'An adorable little boy is spreading'** 다음에 나올 단어를 예측하고 싶다고 할 때, n=4라고 한 4-gram을 이용한 언어 모델을 사용한다고 합시다. 이 경우, spreading 다음에 올 단어를 예측하는 것은 n-1에 해당되는 앞의 3개의 단어만을 고려합니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/62989036-76850b00-be81-11e9-9ca3-27195b360309.png)\n",
    "\n",
    "만약 갖고있는 코퍼스에서 boy is spreading가 1,000번 등장했다고 합시다. 그리고 boy is spreading insults가 500번 등장했으며, boy is spreading smiles가 200번 등장했다고 합시다. 그렇게 되면 boy is spreading 다음에 insults가 등장할 확률은 50%이며, smiles가 등장할 확률은 20%입니다. 확률적 선택에 따라 우리는 insults가 더 맞다고 판단하게 됩니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/62989085-9c121480-be81-11e9-9544-f96e241ed8e0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. N-gram Language Model의 한계**\n",
    "\n",
    "앞서 4-gram을 통한 언어 모델의 동작 방식을 확인했습니다. 그런데 조금 의문이 남습니다. 앞서 본 4-gram 언어 모델은 주어진 문장에서 앞에 있던 단어인 **'작고 사랑스러운(an adorable little)'**이라는 수식어를 제거하고, 반영하지 않았습니다. 그런데 **'작고 사랑스러운'** 수식어까지 모두 고려하여 **작고 사랑하는 소년**이 하는 행동에 대해 다음 단어를 예측하는 언어 모델이었다면 과연 '작고 사랑스러운 소년이' **'모욕을 퍼트렸다'**라는 부정적인 내용이 **'웃음 지었다'**라는 긍정적인 내용 대신 선택되었을까요?\n",
    "\n",
    "물론 코퍼스 데이터를 어떻게 가정하느냐의 나름이고, 전혀 말이 안 되는 문장은 아니지만 여기서 지적하고 싶은 것은 n-gram은 뒤의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 생긴다는 점입니다. 문장을 읽다 보면 앞 부분과 뒷부분의 문맥이 전혀 연결 안 되는 경우도 생길 수 있습니다. 결론만 말하자면, 전체 문장을 고려한 언어 모델보다는 정확도가 떨어질 수밖에 없습니다. 이를 토대로 n-gram 모델에 대한 한계점을 정리해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1) n을 선택하는 것은 trade-off 문제.**\n",
    "앞에서 몇 개의 단어를 볼지 임의의 개수를 정하는 것은 trade-off가 존재합니다. 임의의 개수인 n을 1보다는 2로 선택하는 것은 거의 대부분의 경우에서 언어 모델의 성능을 높일 수 있습니다. 가령, spreading만 보는 것보다는 is spreading을 보고 다음 단어를 예측하는 것이 더 정확하기 때문입니다. 이 경우 훈련 데이터가 적절한 데이터였다면 언어 모델이 적어도 spreading 다음에 동사를 고르지 않을 것입니다.\n",
    "\n",
    "n을 너무 크게 선택하면 실제 훈련 코퍼스에서 해당 n-gram을 카운트할 수 있는 확률은 적어집니다. 즉, n-gram에 대한 OOV(Out Of Vocabulary) 문제가 발생할 수 있습니다. 또한 n이 커질수록 모델 사이즈는 굉장히 커지게 됩니다. 기본적으로 이 모델이 작동되려면 코퍼스의 모든 n-gram에 대해서 카운트를 해야 하기 때문입니다.\n",
    "\n",
    "n을 너무 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 점점 실제의 확률분포와 멀어집니다. 그렇기 때문에 적절한 개수를 선택해야 합니다. 앞서 언급한 trade-off 문제로 인해 정확도를 높이려면 **n은 최대 5를 넘게 잡아서는 안 된다고 권장**되고 있습니다.\n",
    "\n",
    "n이 성능에 영향을 주는 것을 확인할 수 있는 유명한 예제 하나를 보겠습니다. 스탠퍼드 대학교의 공유 자료에 따르면, 월스트리트 저널에서 3,800만 개의 단어 토큰에 대하여 n-gram 언어 모델을 학습하고, 1,500만 개의 테스트 데이터에 대해서 테스트를 했을 때 다음과 같은 성능이 나왔다고 합니다. 뒤에서 배우겠지만, 펄플렉서티(Perplexity)는 수치가 낮을수록 더 좋은 성능을 나타냅니다.\n",
    "\n",
    "|       -        | **Unigram** | **Bigram** | **Trigram** |\n",
    "| :------------: | :---------: | :--------: | :---------: |\n",
    "| **Perplexity** |     962     |    170     |     109     |\n",
    "\n",
    "위의 결과는 n을 1에서 2, 2에서 3으로 올릴 때마다 성능이 올라가는 것을 보여줍니다.\n",
    "\n",
    "\n",
    "### **(2) 카운트 했을 때 0이 되는 문제(zero count problem)**\n",
    "문장에 존재하는 앞에 나온 단어를 모두 보는 것은 현실적으로 코퍼스에서 카운트 할 수 있는 확률이 적기 때문에, n-gram 언어 모델을 사용한다고 언급했습니다. 하지만 n-gram 언어 모델도 여전히 n-gram에 대한 OOV 문제가 존재합니다. 결국 n-gram 언어 모델도, 확률이 0이 되는 또는 확률 자체를 계산할 수 없는 문제를 완전히 피할 수는 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. 적용 분야(Domain)에 맞는 코퍼스의 수집**\n",
    "어떤 분야인지, 어떤 어플리케이션인지에 따라서 특정 단어들의 확률 분포는 당연히 다릅니다. 가령, 마케팅 분야에서는 마케팅 단어가 빈번하게 등장할 것이고, 의료 분야에서는 의료 관련 단어가 당연히 빈번하게 등장합니다. 이 경우 언어 모델에 사용하는 코퍼스를 해당 도메인의 코퍼스를 사용한다면 당연히 언어 모델이 제대로 된 언어 생성을 할 가능성이 높아집니다.\n",
    "\n",
    "때로는 이를 언어 모델의 약점이라고 하는 경우도 있는데, 훈련에 사용된 도메인 코퍼스가 무엇이냐에 따라서 성능이 비약적으로 달라지기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. 인공 신경망을 이용한 언어 모델(Neural Network Based Language Model)**\n",
    "여기서는 다루지 않겠지만, N-gram Language Model의 한계점을 극복하기위해 분모, 분자에 숫자를 더해서 카운트했을 때 0이 되는 것을 방지하는 등의 여러 일반화(generalization) 방법들이 존재합니다. 하지만 그럼에도 본질적으로 n-gram 언어 모델에 대한 취약점을 완전히 해결하지는 못하였고, 이를 위한 대안으로 N-gram Language Model보다 대체적으로 성능이 우수한 **인공 신경망을 이용한 언어 모델**이 많이 사용되고 있습니다. **인공 신경망을 이용한 언어 모델**은 RNN이나 seq2seq 등 딥 러닝 챕터에서 다루게 될 예정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **4) 한국어에서의 언어 모델(Language Model for Korean Sentences)**\n",
    "\n",
    "지금까지 언어 모델에 대해서 배워보았습니다. 그런데 영어나 기타 언어에 비해서 한국어는 언어 모델로 다음 단어를 예측하기가 훨씬 까다롭습니다. 사실, 대부분은 이미 토큰화에서 언급했던 내용이기도 한데, 적절한 토큰화가 언어 모델에서 필수적이기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 한국어는 어순이 중요하지 않다.**\n",
    "\n",
    "한국어에서는 어순이 중요하지 않습니다. 그래서 이전 단어가 주어졌을때, 다음 단어가 나타날 확률을 구해야하는데 어순이 중요하지 않다는 것은 어떤 단어든 나타나도 된다는 의미입니다.\n",
    "\n",
    "예를 들어보도록 합시다.\n",
    "\n",
    "> Ex) <br>\n",
    "> ① 나는 운동을 합니다 체육관에서. <br>\n",
    "> ② 나는 체육관에서 운동을 합니다. <br>\n",
    "> ③ 체육관에서 운동을 합니다. <br>\n",
    "> ④ 나는 운동을 체육관에서 합니다. <br>\n",
    "\n",
    "4개의 문장은 전부 의미가 통하는 것을 볼 수 있습니다. 심지어 '나는' 이라는 주어를 생략해도 말이 되버립니다. 이렇게 단어 순서를 뒤죽박죽으로 바꾸어놔도 한국어는 의미가 전달 되기 때문에 확률에 기반한 언어 모델이 제대로 다음 단어를 예측하기가 어렵습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 한국어는 교착어이다.**\n",
    "\n",
    "한국어는 교착어입니다. 이는 한국어에서의 언어 모델 작동을 어렵게 만듭니다. 띄어쓰기 단위로 토큰화를 할 경우에는 문장에서 발생가능한 단어의 수가 굉장히 늘어납니다. 대표적인 예로 교착어인 한국어에는 조사가 있습니다. 영어는 기본적으로 조사가 없습니다. 하지만 한국어에는 어떤 행동을 하는 동사의 주어나 목적어를 위해서 조사라는 것이 있습니다.\n",
    "\n",
    "가령 '그녀'라는 단어 하나만 해도 그녀가, 그녀를, 그녀의, 그녀와, 그녀로, 그녀께서, 그녀처럼 등과 같이 다양한 경우가 존재합니다. 그렇기 때문에, 한국어에서는 **토큰화**를 통해 접사나 조사 등을 분리하는 것은 중요한 작업이 되기도 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. 한국어는 띄어쓰기가 제대로 지켜지지 않는다.**\n",
    "\n",
    "한국어는 띄어쓰기를 제대로 하지 않아도 의미가 전달되며, 띄어쓰기 규칙 또한 상대적으로 까다로운 언어이기 때문에 자연어 처리를 하는 것에 있어서 한국어 코퍼스는 띄어쓰기가 제대로 지켜지지 않는 경우가 많습니다. 이 경우에 토큰이 제대로 분리 되지 않아 언어 모델은 제대로 동작하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
