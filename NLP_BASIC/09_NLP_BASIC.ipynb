{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **머신 러닝(Machine Learning) 개요**\n",
    "  AI가 최근 몇 년간 4차 산업혁명의 주 키워드로 언급되고 있습니다. AI는 여러 가지 의미를 포괄하고 있지만, 지금에 이르러 사람들이 말하는 AI는 바로 머신 러닝(Machine Learning)과 머신 러닝의 한 갈래인 딥 러닝(Deep Learning)을 의미합니다. 혹자는 규칙을 찾아 프로그래밍을 하는 전통적인 프로그래밍 방법에서, 스스로 규칙을 찾아내가는 머신 러닝이 새로운 프로그래밍의 패러다임이 될 것이라고 말하기도 합니다. 그만큼 머신 러닝은 기존에 해결할 수 없었던 수많은 문제들에 대한 최적의 해결방법을 제시해주고 있습니다.\n",
    "\n",
    "  머신 러닝은 이미지 인식, 영상 처리, 알파고와 같은 분야 뿐만 아니라 자연어 처리에 있어서도 유용하게 쓰입니다. 특히 머신 러닝의 한 갈래인 딥 러닝은 기존의 통계 기반에서 접근했던 자연어 처리의 성능을 훨씬 뛰어 넘는 성능을 보이고 있어, 현재에 이르러서는 자연어 처리에 있어 딥 러닝은 필수가 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **1) 머신 러닝이란(What is Machine Learning?)**\n",
    "\n",
    "이번 챕터에서는 딥 러닝을 포함하고 있는 개념인 머신 러닝(Machine Learning)에 대한 개념에 대해서 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 머신 러닝(Machine Learning)이 아닌 접근 방법의 한계**\n",
    "\n",
    "머신 러닝을 이해하기 위해 머신 러닝이 아닌, 기존의 프로그래밍 접근 방법을 통해 프로그램을 작성했을 때 한계가 있는 경우를 예로 들어보겠습니다. <br>\n",
    "\n",
    "- **Ex) 주어진 사진으로부터 고양이 사진인지 강아지 사진인지 판별하는 일.** <br>\n",
    "  위 문제는 실제로 2017년에 있었던 DGIST의 딥 러닝 경진대회의 문제임을 밝힙니다.\n",
    "\n",
    "사람에게 있어 위와 같은 일은 어렵지 않습니다. 직관적으로 고양이 사진인지, 강아지 사진인지 아주 쉽게 알 수 있습니다. 그런데 프로그램. 즉, 기계에게 이러한 일은 쉽지 않습니다. 그 이유를 알아봅시다.\n",
    "\n",
    "해당 프로그램은 이미지라는 것을 입력으로 받아서, 이미지가 강아지에 속하는지 고양이에 속하는지의 결과값을 리턴하는 프로그램이 될 것입니다.\n",
    "\n",
    "```python\n",
    "def prediction(이미지 as input):\n",
    "\t어떻게 코딩해야하지?\n",
    "\treturn 결과\n",
    "```\n",
    "\n",
    "그런데 사실 구글에서 강아지와 고양이 사진을 검색해보면 알겠지만, 사진이란 건 사진을 보는 각도, 조명, 타겟의 변형(고양이의 자세)에 따라서 너무나 천차만별이라 사진으로부터 **공통된 명확한 특징을 잡아내는 것이 쉽지 않습니다.**\n",
    "\n",
    "결론을 미리 언급하자면 해당 프로그램은 숫자를 정렬하는 것과 같은 명확한 알고리즘이 애초에 존재하지 않습니다.\n",
    "\n",
    "![img](https://wikidocs.net/images/page/21679/%EA%B3%A0%EC%96%91%EC%9D%B4_%EC%95%A1%EC%B2%B42.jpg)\n",
    "\n",
    "![img](https://wikidocs.net/images/page/21679/%EA%B3%A0%EC%96%91%EC%9D%B4_%EC%97%89%EB%8D%A9%EC%9D%B4.jpg)\n",
    "\n",
    "역사적으로 이러한 이미지 인식 분야에서 특징을 잡아내기 위한 시도들이 있었습니다. 이미지의 shape나 edge와 같은 것들을 찾아내서 알고리즘화 하려고 시도하고, 다른 사진 이미지가 들어오면 전반적인 상태를 비교하여 분류하려고 한 것입니다.\n",
    "\n",
    "하지만 그러한 시도들은 위에서 언급한 이유로 결국 특징을 잡아내는 것에 한계가 있을 수밖에 없었습니다. 위와 같은 예제를 언급한 이유는 머신 러닝은 이에 대한 해결책이 될 수 있기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 머신 러닝은 기존 프로그래밍의 한계에 대한 해결책이 될 수 있다.**\n",
    "\n",
    "![img](https://wikidocs.net/images/page/21679/%EC%A0%84%ED%86%B5_vs_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D.png)\n",
    "\n",
    "머신 러닝이 위의 문제를 해결할 수 있는 이유는 해결을 위한 접근 방법이 앞서 언급한 기존의 프로그래밍과는 다르기 때문입니다. 위의 이미지에서 위쪽은 기존의 프로그래밍 방법으로 접근한 것이고, 아래쪽은 머신 러닝의 접근 방법을 보여줍니다.\n",
    "\n",
    "머신 러닝은 주어진 데이터로부터 결과를 찾는 것에 초점을 맞추는 것이 아니라, 주어진 데이터로부터 규칙성을 찾는 것에 초점이 맞추어져 있습니다. 주어진 데이터로부터 규칙성을 찾는 과정을 우리는 학습(training)이라고 합니다.\n",
    "\n",
    "일단 규칙성을 발견해내면, 그 후에 들어오는 새로운 데이터에 대해서 발견한 규칙성을 기준으로 정답을 찾아내는데, 이는 기존의 프로그래밍 방식으로 접근하기 어려웠던 문제의 해결책이 되기도 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **2) 머신 러닝 훑어보기**\n",
    "이번 챕터에서는 머신 러닝의 특징들에 대해서 배웁니다. 딥 러닝 또한 머신 러닝에 속하므로, 아래의 머신 러닝의 특징들은 모두 딥 러닝의 특징이기도 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 머신 러닝 모델의 평가**\n",
    "\n",
    "![img](https://wikidocs.net/images/page/24987/%EB%8D%B0%EC%9D%B4%ED%84%B0.PNG)\n",
    "\n",
    "실제 모델을 평가하기 위해서는 데이터를 훈련용, 검증용, 테스트용 이렇게 세 가지로 분리하는 것이 일반적입니다. 다만, 이 책의 목적은 개념 학습이므로 이 책의 대부분의 실습에서는 별도로 세 가지로 분리하지 않고 훈련용, 테스트용으로만 분리해서 사용합니다. 하지만 실제로는 그렇게 하지 않는 것이 좋다는 점을 설명하고자 이 내용을 넣었습니다. 그렇다면 훈련용, 테스트용 두 가지로만 나눠서 테스트 데이터로 한 번만 테스트하면 더 편할텐데 굳이 왜 검증용 데이터를 만들어 놓는 것일까요?\n",
    "\n",
    "검증용 데이터는 모델의 성능을 평가하기 위한 용도가 아니라, 모델의 성능을 조정하기 위한 용도입니다. 더 정확히는 과적합이 되고 있는지 판단하거나 하이퍼파라미터의 조정을 위한 용도입니다. **하이퍼파라미터**란 값에 따라서 모델의 성능에 영향을 주는 매개변수들을 말합니다. 하이퍼파라미터를 해석하면 초매개변수라고 할 수도 있지만, 하이퍼파라미터란 이름으로 더 많이 불리므로 이 책에서는 하이퍼파라미터라고 명명합니다. 반면, 가중치와 편향과 같은 학습을 통해 바뀌어져가는 변수를 이 책에서는 **매개변수**라고 부릅니다.\n",
    "\n",
    "이 두 값 **하이퍼파라미터**와 **매개변수**의 가장 큰 차이는 하이퍼파라미터는 보통 사용자가 직접 정해줄 수 있는 변수라는 점입니다. 뒤의 선형 회귀 챕터에서 배우게 되는 경사 하강법에서 학습률(learning rate)이 이에 해당되며 딥 러닝에서는 은닉층의 수, 뉴런의 수, 드롭아웃 비율 등이 이에 해당됩니다. 반면 여기서 언급하는 매개변수는 사용자가 결정해주는 값이 아니라 모델이 학습하는 과정에서 얻어지는 값입니다. 정리하면 절대적인 정의라고는 할 수 없지만, 하이퍼파라미터는 사람이 정하는 변수인 반면, 매개변수는 기계가 훈련을 통해서 바꾸는 변수라고 할 수 있으며 이 책에서는 이와 같은 기준으로 변수의 이름을 명명합니다.\n",
    "\n",
    "훈련용 데이터로 훈련을 모두 시킨 모델은 검증용 데이터를 사용하여 정확도를 검증하며 하이퍼파라미터를 **튜닝(tuning)**합니다. 또한 이 모델의 매개변수는 검증용 데이터로 정확도가 검증되는 과정에서 점차 검증용 데이터에 점점 맞추어져 가기 시작합니다.\n",
    "\n",
    "하이퍼파라미터 튜닝이 끝났다면, 이제 검증용 데이터로 모델을 평가하는 것은 적합하지 않습니다. 이제 모델은 검증용 데이터에 대해서도 일정 부분 최적화가 되어있기 때문입니다. 모델에 대한 평가는 모델이 아직까지 보지 못한 데이터로 하는 것이 가장 바람직합니다. 검증이 끝났다면 테스트 데이터를 가지고 모델의 진짜 성능을 평가합니다. 비유하자면 훈련 데이터는 문제지, 검증 데이터는 모의고사, 테스트 데이터는 실력을 최종적으로 평가하는 수능 시험이라고 볼 수 있습니다.\n",
    "\n",
    "만약, 검증 데이터와 테스트 데이터를 나눌 만큼 데이터가 충분하지 않다면 k-폴드 교차 검증이라는 또 다른 방법을 사용하기도 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 분류(Classification)와 회귀(Regression)**\n",
    "\n",
    "전부라고는 할 수 없지만, 머신 러닝의 많은 문제는 분류 또는 회귀 문제에 속합니다. 이 책의 머신 러닝 챕터에서는 머신 러닝 기법 중 선형 회귀(Lineare Regression)과 로지스틱 회귀(Logistic Rgression)를 다루는데 선형 회귀를 통해 회귀 문제에 대해서 학습하고, 로지스틱 회귀를 통해 (이름은 회귀이지만) 분류 문제를 학습합니다.\n",
    "\n",
    "분류는 또한 이진 분류(Binary Classification)과 다중 클래스 분류(Multi-Class Classification)로 나뉩니다. 엄밀히는 다중 레이블 분류(Multi-lable Classification)라는 또 다른 문제가 존재하지만, 이 책에서는 이진 분류와 다중 클래스 분류만 다룹니다.\n",
    "\n",
    "### **1) 이진 분류 문제(Binary Classification)**\n",
    "\n",
    "이진 분류는 주어진 입력에 대해서 **둘 중 하나의 답을 정하는 문제**입니다. 시험 성적에 대해서 합격, 불합격인지 판단하고 메일로부터 정상 메일, 스팸 메일인지를 판단하는 문제 등이 이에 속합니다.\n",
    "\n",
    "### **2) 다중 클래스 분류(Multi-class Classification)**\n",
    "\n",
    "다중 클래스 분류는 **주어진 입력에 대해서 두 개 이상의 정해진 선택지 중에서 답을 정하는 문제**입니다. 예를 들어 서점 아르바이트를 하는데 과학, 영어, IT, 학습지, 만화라는 레이블이 각각 붙여져 있는 5개의 책장이 있다고 합시다. 새 책이 입고되면, 이 책은 다섯 개의 책장 중에서 분야에 맞는 적절한 책장에 책을 넣어야 합니다. 이 때의 다섯 개의 선택지를 주로 카테고리 또는 범주 또는 클래스라고 하며, 주어진 입력으로부터 정해진 클래스 중 하나로 판단하는 것을 다중 클래스 분류 문제라고 합니다.\n",
    "\n",
    "### **3) 회귀 문제(Regression)**\n",
    "\n",
    "회귀 문제는 분류 문제처럼 0 또는 1이나 과학 책장, IT 책장 등과 같이 분리된(비연속적인) 답이 결과가 아니라 **연속된 값을 결과로 가집니다.** 예를 들어 시험 성적을 예측하는데 5시간 공부하였을 때 80점, 5시간 1분 공부하였을 때는 80.5점, 7시간 공부하였을 때는 90점 등이 나오는 것과 같은 문제가 있습니다. 그 외에도 시계열 데이터를 이용한 주가 예측, 생산량 예측, 지수 예측 등이 이에 속합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. 지도 학습(Supervised Learning)과 비지도 학습(Unsupervised Learning)**\n",
    "\n",
    "머신 러닝은 크게 지도 학습, 비지도 학습, 강화 학습으로 나눕니다. 하지만 강화 학습은 이 책의 범위를 벗어나므로 설명하지 않습니다. 또한 이 책은 주로 지도 학습에 대해서 다룹니다.\n",
    "\n",
    "### **1) 지도 학습**\n",
    "\n",
    "지도 학습이란 레이블(Label)이라는 정답과 함께 학습하는 것을 말합니다. 이는 앞서 2챕터의 데이터의 분리 챕터에서 상세히 설명한 바 있습니다. 레이블이라는 말 외에도 yy, 실제값 등으로 부르기도 하는데 이 책에서는 이 용어들을 상황에 따라서 바꿔서 사용합니다.\n",
    "\n",
    "이때 기계는 예측값과 실제값의 차이인 오차를 줄이는 방식으로 학습을 하게 되는데 예측값은 y^y^과 같이 표현하기도 합니다.\n",
    "\n",
    "### **2) 비지도 학습**\n",
    "\n",
    "반면, 비지도 학습은 레이블이 없이 학습하는 것을 말합니다. 예를 들어 토픽 모델링의 LDA는 비지도 학습에 속하며, 뒤에서 배우게 되는 워드투벡터(Word2Vec)는 마치 지도 학습을 닮았지만, 비지도 학습에 속합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. 샘플(Sample)과 특성(Feature)**\n",
    "\n",
    "많은 머신 러닝 문제가 1개 이상의 독립 변수 xx를 가지고 종속 변수 yy를 예측하는 문제입니다. 많은 머신 러닝 모델들, 특히 인공 신경망 모델은 독립 변수, 종속 변수, 가중치, 편향 등을 행렬 연산을 통해 연산하는 경우가 많습니다. 그래서 앞으로 인공 신경망을 배우게되면 훈련 데이터를 행렬로 표현하는 경우를 많이 보게 될 겁니다. 독립 변수 xx의 행렬을 X라고 하였을 때, 독립 변수의 개수가 n개이고 데이터의 개수가 m인 행렬 X는 다음과 같습니다.\n",
    "\n",
    "![img](https://wikidocs.net/images/page/35821/n_x_m.PNG)\n",
    "\n",
    "이때 머신 러닝에서는 하나의 데이터, 하나의 행을 샘플(Sample)이라고 부릅니다. (데이터베이스에서는 레코드라고 부르는 단위입니다.) 종속 변수 yy를 예측하기 위한 각각의 독립 변수 xx를 특성(Feature)이라고 부릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. 혼동 행렬(Confusion Matrix)**\n",
    "\n",
    "머신 러닝에서는 맞춘 문제수를 전체 문제수로 나눈 값을 정확도(Accuracy)라고 합니다. 하지만 정확도는 맞춘 결과와 틀린 결과에 대한 세부적인 내용을 알려주지는 않습니다. 이를 위해서 사용하는 것이 혼동 행렬(Confusion Matrix)입니다.\n",
    "\n",
    "예를 들어 양성(Positive)과 음성(Negative)을 구분하는 이진 분류가 있다고 하였을 때 혼동 행렬은 다음과 같습니다. **각 열은 예측값**을 나타내며, **각 행은 실제값**을 나타냅니다.\n",
    "\n",
    "|  -   |  참  | 거짓 |\n",
    "| :--: | :--: | :--: |\n",
    "|  참  |  TP  |  FN  |\n",
    "| 거짓 |  FP  |  TN  |\n",
    "\n",
    "이를 각각 TP(True Positive), TN(True Negative), FP(False Postivie), FN(False Negative)라고 하는데 True는 정답을 맞춘 경우고 False는 정답을 맞추지 못한 경우입니다. 그리고 Positive와 Negative는 각각 제시했던 정답입니다. 즉, TP는 양성(Postive)이라고 대답하였고 실제로 양성이라서 정답을 맞춘 경우입니다. TN은 음성(Negative)이라고 대답하였는데 실제로 음성이라서 정답을 맞춘 경우입니다.\n",
    "\n",
    "그럼 FP는 양성이라고 대답하였는데, 음성이라서 정답을 틀린 경우이며 FN은 음성이라고 대답하였는데 양성이라서 정답을 틀린 경우가 됩니다. 그리고 이 개념을 사용하면 또 새로운 개념인 정밀도(Precision)과 재현률(Recall)이 됩니다.\n",
    "\n",
    "### **1) 정밀도(Precision)**\n",
    "\n",
    "정밀도은 양성이라고 대답한 전체 케이스에 대한 TP의 비율입니다. 즉, 정밀도를 수식으로 표현하면 다음과 같습니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/63658020-378c7900-c7e2-11e9-915c-e8b464864f18.png)\n",
    "\n",
    "### **2) 재현률(Recall)**\n",
    "\n",
    "재현률은 실제값이 양성인 데이터의 전체 개수에 대해서 TP의 비율입니다. 즉, 양성인 데이터 중에서 얼마나 양성인지를 예측(재현)했는지를 나타냅니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/63658022-42470e00-c7e2-11e9-9d62-c91a15b94d7c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. 과적합(Overfitting)과 과소 적합(Underfitting)**\n",
    "\n",
    "학생의 입장이 되어 같은 문제지를 과하게 많이 풀어서 문제 번호만 봐도 정답을 맞출 수 있게 되었다고 가정합시다. 그런데 다른 문제지나 시험을 보면 점수가 안 좋다면 그게 의미가 있을까요?\n",
    "\n",
    "머신 러닝에서 **과적합(Overfitting)**이란 훈련 데이터를 과하게 학습한 경우를 말합니다. 훈련 데이터는 실제로 존재하는 많은 데이터의 일부에 불과합니다. 그런데 기계가 훈련 데이터에 대해서만 과하게 학습하면 테스트 데이터나 실제 서비스에서의 데이터에 대해서는 정확도가 좋지 않은 현상이 발생합니다.\n",
    "\n",
    "예를 들어 강아지 사진과 고양이 사진을 구분하는 기계가 있을 때, 검은색 강아지 사진 훈련 데이터를 과하게 학습하면 기계는 나중에 가서는 흰색 강아지나, 갈색 강아지를 보고도 강아지가 아니라고 판단하게 됩니다. 이는 훈련 데이터에 대해서 지나친 일반화를 한 상황입니다.\n",
    "\n",
    "과적합 상황에서는 훈련 데이터에 대해서는 오차가 낮지만, 테스트 데이터에 대해서는 오차가 높아지는 상황이 발생합니다. 아래의 그래프는 과적합 상황에서 발생할 수 있는 훈련 횟수에 따른 훈련 데이터의 오차와 테스트 데이터의 오차의 변화를 보여줍니다.\n",
    "\n",
    "![img](https://wikidocs.net/images/page/32012/%EC%8A%A4%ED%8C%B8_%EB%A9%94%EC%9D%BC_%EC%98%A4%EC%B0%A8.png)\n",
    "\n",
    "사실 위의 그래프는 10챕터에서 배우게 될 스팸 메일 분류하기 실습에서 훈련 데이터에 대한 훈련 횟수를 30회로 주어서 의도적으로 과적합을 발생시켰을 때의 그래프입니다. X축의 에포크(epoch)는 전체 훈련 데이터에 대한 훈련 횟수를 의미합니다. 앞으로 계속 나오게 될 용어이니 기억합시다.\n",
    "\n",
    "스팸 메일 분류하기 실습은 에포크가 3~4를 넘어가게 되면 과적합이 발생합니다. 위의 그래프는 테스트 데이터에 대한 오차가 점차 증가하는 양상을 보여줍니다. 반대로 말하면 훈련 데이터에 대한 정확도는 높지만, 테스트 데이터는 정확도가 낮은 상황이라고 말할 수도 있습니다. 즉, 테스트 데이터의 오차가 증가하기 전이나, 정확도가 감소하기 전에 훈련을 멈추는 것이 바람직합니다.\n",
    "\n",
    "과적합 방지를 위해 테스트 데이터의 성능이 낮아지기 전에 훈련을 멈추는 것이 바람직하다고 했는데, 테스트 데이터의 성능이 올라갈 여지가 있음에도 훈련을 덜 한 상태를 반대로 과소적합(Underfitting)이라고 합니다. 과소 적합은 훈련 자체가 부족한 상태이므로 과대 적합과는 달리 훈련 데이터에 대해서도 보통 정확도가 낮다는 특징이 있습니다.\n",
    "\n",
    "이러한 두 가지 현상을 과적합과 과소 적합이라고 부르는 이유는 머신 러닝에서 학습 또는 훈련이라고 하는 과정을 적합(fitting)이라고도 부를 수 있기 때문입니다. 모델이 주어진 데이터에 대해서 적합해져가는 과정이기 때문입니다. (이러한 이유로 케라스에서는 기계를 학습시키는 도구의 이름을 fit()이라고 명명합니다.)\n",
    "\n",
    "딥 러닝을 할 때는 과적합을 막을 수 있는 드롭 아웃(Drop out), 조기 종료(Early Stopping)과 같은 몇 가지 방법이 존재하는데 이는 인공 신경망 챕터에서 소개합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **3) 선형 회귀(Linear Regression)**\n",
    "딥 러닝을 이해하기 위해서는 선형 회귀(Linear Regression)와 로지스틱 회귀(Logsitic Regression)를 이해할 필요가 있습니다. 이번 챕터에서는 우선 선형 회귀에 대해서 이해해보도록 하겠습니다. 또한 선형 회귀 개념 자체에 대한 이해도 중요하지만, 또한 머신 러닝에서 쓰이는 용어인 가설(Hypothesis), 손실 함수(Loss Function) 그리고 경사 하강법(Gradient Descent)에 대한 개념을 이해합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 선형 회귀(Linear Regression)**\n",
    "\n",
    "시험 공부하는 시간을 늘리면 늘릴 수록 성적이 잘 나옵니다. 하루에 걷는 횟수를 늘릴 수록, 몸무게는 줄어듭니다. 집의 평수가 클수록, 집의 매매 가격은 비싼 경향이 있습니다. 이는 수학적으로 생각해보면 어떤 요인의 수치에 따라서 특정 요인의 수치가 영향을 받고있다고 말할 수 있습니다. 조금 더 수학적인 표현을 써보면 어떤 변수의 값에 따라서 특정 변수의 값이 영향을 받고 있다고 볼 수 있습니다. 다른 변수의 값을 변하게하는 변수를 x, 변수 x에 의해서 값이 종속적으로 변하는 변수 y라고 해봅시다.\n",
    "\n",
    "이때 변수 x의 값은 독립적으로 변할 수 있는 것에 반해, y값은 계속해서 x의 값에 의해서, 종속적으로 결정되므로 x를 독립 변수, y를 종속 변수라고 합니다. 위의 예에서 시험 공부 시간은 독립 변수 x, 성적은 종속 변수 y라고 할 수 있겠습니다.\n",
    "\n",
    "선형 회귀는 종속 변수 y와 한 개 이상의 독립 변수 x와의 선형 관계를 모델링하는 분석 기법입니다. 즉, x는 1개가 아니라 그 이상일 수 있습니다. 만약, 독립 변수 x가 1개라면 단순 선형 회귀라고 합니다.\n",
    "\n",
    "### **1) 단순 선형 회귀 분석(Simple Linear Regression Analysis)**\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/63739580-ac39e300-c8c8-11e9-875c-08f423d6ef84.png)\n",
    "\n",
    "위의 수식은 단순 선형 회귀의 수식을 보여줍니다. 여기서 독립 변수 x와 곱해지는 값 W를 머신 러닝에서는 가중치(Weight),별도로 더해지는 값 b를 편향(Bias)이라고 합니다. 직선의 방정식에서는 각각 직선의 기울기와 절편을 의미합니다. W와 b가 왜 필요할까요? W와 b가 없이 y와 x란 수식은 y는 x와 같다는 하나의 식밖에 표현하지 못합니다. 그래프 상으로 말하면, 하나의 직선밖에 표현하지 못합니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/63739766-75b09800-c8c9-11e9-987b-df230ebd82a5.png)\n",
    "\n",
    "여러 다양한 직선을 표현하기 위해서는 W와 b가 필요합니다. W와 b가 x와 y의 관계를 모델링하는 값입니다. W와 b의 값을 엉망으로 찾으면 x와 y의 관계를 찾아내지 못합니다. 하지만 W와 b의 값을 제대로 찾아내면 x와 y의 관계를 제대로 찾아낸 것이 됩니다.\n",
    "\n",
    "### **2) 다중 선형 회귀 분석(Multiple Linear Regression Analysis)**\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/63739805-94169380-c8c9-11e9-85cb-10c39db7dbe9.png)\n",
    "\n",
    "잘 생각해보니까 집의 매매 가격은 단순히 집의 평수가 크다고 결정되는 게 아닙니다.\n",
    "\n",
    "- ***집의 층의 수***\n",
    "- ***방의 개수***\n",
    "- ***지하철 역과의 거리***\n",
    "\n",
    "위의 사항도 영향이 있는 것 같습니다. 이제 이러한 다수의 요소를 가지고 집의 매매 가격을 예측해보고 싶습니다. y는 여전히 1개이지만 이제 x는 1개가 아니라 여러 개가 되었습니다. 이제 이를 다중 선형 회귀 분석이라고 합니다. 이에 대한 실습은 뒤의 챕터에서 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 가설(Hypothesis) 세우기**\n",
    "\n",
    "**단순 선형 회귀**를 가지고 실제로 문제를 풀어보겠습니다. 어떤 학생이 공부 시간에 따라서 다음과 같은 점수를 얻었다는 데이터가 있습니다. 공부 시간이 x라면, 점수는 y입니다.\n",
    "\n",
    "| hours(x) | score(y) |\n",
    "| :------: | :------: |\n",
    "|    2     |    25    |\n",
    "|    3     |    50    |\n",
    "|    4     |    42    |\n",
    "|    5     |    61    |\n",
    "\n",
    "이를 좌표 평면에 그려보면 다음과 같습니다. 단, 아래의 그래프에서 x와 y에 붙은 숫자는 서로 다른 독립 변수와 종속 변수를 의미하는 것이 아니라, 단순 선형 회귀에서의 서로 다른 값을 의미합니다.\n",
    "\n",
    "![img](https://wikidocs.net/images/page/21670/%EC%A0%90%EC%86%8C%EB%AC%B8%EC%9E%90%EB%A1%9C%EB%8B%A4%EC%8B%9C.PNG)\n",
    "\n",
    "알려준 데이터로부터 x와 y의 관계를 유추하고, 이 학생이 6시간을 공부하였을 때의 성적, 그리고 7시간, 8시간을 공부하였을 때의 성적을 예측해보고 싶습니다. x와 y의 관계를 유추하기 위해서 수학적으로 식을 세워보게 되는데 이 때 머신 러닝에서는 y와 x간의 관계를 유추한 식을 **가설(Hypothesis)**이라고 합니다. 아래의 H(x)에서 HH는 Hypothesis를 의미합니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/63740030-6e3dbe80-c8ca-11e9-8e29-51a6fd0ae04f.png)\n",
    "\n",
    "![img](https://wikidocs.net/images/page/21670/W%EC%99%80_b%EA%B0%80_%EB%8B%A4%EB%A6%84.PNG)\n",
    "\n",
    "위의 그림은 W와 b의 값에 따라서 천차만별로 그려지는 직선의 모습을 보여줍니다. (직선의 방정식을 알고있다면, 위의 가설에서 W는 직선의 기울기이고 b는 절편으로 직선을 표현함을 알 수 있습니다.) 결국 선형 회귀는 주어진 데이터로부터 y와 x의 관계를 가장 잘 나타내는 직선을 그리는 일을 말합니다. 그리고 어떤 직선인지 결정하는 것은 W와 b의 값이므로 선형 회귀에서 해야할 일은 결국 적절한 W와 b를 찾아내는 일이 됩니다. (사실 AI 알고리즘이 하는 것이 바로 적절한 W와 b를 찾아내는 일입니다.)\n",
    "\n",
    "아직 방법을 배우지는 않았지만, 어떤 방법을 사용하여 적절한 W와 b의 값을 찾은 덕택에 y와 x의 관계를 가장 잘 나타내는 직선을 위의 좌표 평면 상에서 그렸다고 한 번 가정해보겠습니다. 이 직선을 x가 6일때, 7일때, 8일때에 대해서도 계속해서 직선을 그저 이어그린다면 이 학생이 6시간을 공부했을 때, 7시간을 공부했을 때, 8시간을 공부했을 때의 예상 점수를 말할 수 있게 됩니다. 왜냐면 x가 각각 6일 때, 7일 때, 8일 때의 y값을 확인하면 되기 때문입니다.\n",
    "\n",
    "그렇다면, 어떻게 적절한 W와 b를 찾을 수 있을까요? 지금부터 W와 b를 구하는 방법에 대해서 배워보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***일차함수 개념 강의 영상***\n",
    "\n",
    "- [중2 수학개념] 일차함수의 뜻과 그래프 (수박씨닷컴 중등 인강)\n",
    "  - https://www.youtube.com/watch?v=3G42v6mJDiQ\n",
    "- [중2 수학개념] 일차함수의 그래프의 기울기 (수박씨닷컴 중등 인강)\n",
    "  - https://www.youtube.com/watch?v=LM75yPomzz0\n",
    "- [중2 수학개념] 일차함수의 그래프의 절편 (수박씨닷컴 중등 인강)\n",
    "  - https://www.youtube.com/watch?v=VSnFv5suAdk\n",
    "- [중2 수학개념] 일차함수의 그래프의 성질과 평행, 일치 (수박씨닷컴 중등 인강)\n",
    "  - https://www.youtube.com/watch?v=ZqouJucqq9Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. 비용 함수(Cost function) : 평균 제곱 오차(MSE)**\n",
    "\n",
    "앞서 주어진 데이터에서 x와 y의 관계를 W와 b를 이용하여 식을 세우는 일을 가설이라고 언급했습니다. 그리고 이제 해야할 일은 문제에 대한 규칙을 가장 잘 표현하는 W와 b를 찾는 일입니다. 머신 러닝은 W와 b를 찾기 위해서 실제값과 가설로부터 얻은 예측값의 오차를 계산하는 식을 세우고, 이 식의 값을 최소화하는 최적의 W와 b를 찾아냅니다.\n",
    "\n",
    "이 때 실제값과 예측값에 대한 오차에 대한 식을 **목적 함수(Objective function) 또는 비용 함수(Cost function) 또는 손실 함수(Loss function)**라고 합니다. 이 함수들은 출처에 따라서 다르게 불리는 경우가 많으므로 간단히 정의해봅시다.\n",
    "\n",
    "**함수의 값을 최소화하거나, 최대화하거나 하는 목적을 가진 함수**\n",
    "\n",
    "- *목적 함수(Objective function)*\n",
    "\n",
    "**그리고 값을 최소화하려고 하면**\n",
    "\n",
    "- *비용 함수(Cost function)* 또는\n",
    "- *손실 함수(Loss function)*\n",
    "\n",
    "이 책에서는 목적 함수, 비용 함수, 손실 함수란 용어를 같은 의미로 상황에 따라서 혼용해서 사용합니다. 결국 인공 신경망을 공부하면 계속해서 보게되는 용어이므로 모두 숙지합시다.\n",
    "\n",
    "비용 함수는 단순히 실제값과 예측값에 대한 오차를 표현하면 되는 것이 아니라, 예측값의 오차를 줄이는 일에 최적화 된 식이어야 합니다. 앞으로 배우게 될 머신 러닝, 딥 러닝에는 다양한 문제들이 있고, 각 문제들에는 적합한 비용 함수들이 있는데 회귀 문제의 경우에는 주로 **평균 제곱 오차(Mean Squered Error, MSE)**가 사용됩니다. 예를 통해서 이해해보도록 하겠습니다.\n",
    "\n",
    "![img](https://wikidocs.net/images/page/21670/%EC%A0%90%EC%86%8C%EB%AC%B8%EC%9E%90%EB%A1%9C2.PNG)\n",
    "\n",
    "우선 위의 그래프에 임의의 W의 값 13과 임의의 b의 값 1을 가진 직선을 그렸다고 가정합니다. 즉, 아직 정답인 직선이 아니라 랜덤으로 그어 본 직선입니다. 이제 이 직선으로부터 서서히 W와 b의 값을 바꾸면서 정답인 직선을 찾아내야 합니다.\n",
    "\n",
    "사실 y와 X의 관계를 가장 잘 나타내는 직선을 그린다는 것은 위의 그림에서 모든 점들과 위치적으로 가장 가까운 직선을 그린다는 것과 같습니다. 이로부터 오차를 정의하겠습니다. 오차는 주어진 데이터에서 각 X에서의 실제값 y와 위의 직선에서 예측하고 있는 H(x)값의 차이를 말합니다. 즉, 위의 그림에서 ↕는 각 점에서의 오차의 크기를 보여줍니다. 오차를 줄여가면서 W와 b의 값을 찾아내기 위해서는 오차의 크기를 측정할 방법이 필요합니다.\n",
    "\n",
    "오차의 크기를 측정하기 위한 가장 기본적인 방법은 각 오차를 모두 더하는 방법이 있습니다. 위의 y=13x+1 직선이 예측한 예측값을 각각 실제값으로부터 오차를 계산하여 표를 만들어보면 아래와 같습니다.\n",
    "\n",
    "| hours(x) |  2   |  3   |  4   |  5   |\n",
    "| :------: | :--: | :--: | :--: | :--: |\n",
    "|  실제값  |  25  |  50  |  42  |  61  |\n",
    "|  예측값  |  27  |  40  |  53  |  66  |\n",
    "|   오차   |  -2  |  10  |  -7  |  -5  |\n",
    "\n",
    "그런데, 수식적으로 단순히 실제값 - 예측값을 수행하면 오차값이 음수가 나오는 경우가 생깁니다. 이 경우, 오차를 모두 더하면 제대로 된 오차의 크기를 측정할 수 없으므로 보통 오차의 크기를 측정하기 위해서 모든 오차를 제곱하여 더하는 방법을 사용합니다. 즉, 위의 그림에서의 모든 점과 직선 사이의 ↕ 거리를 제곱하고 모두 더합니다. 이를 수식으로 표현하면 아래와 같습니다. 단, 여기서 n은 갖고 있는 데이터의 개수를 의미합니다.\n",
    "\n",
    "* yi = 실제값\n",
    "* H(x1) = 예측값\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/63741330-9b40a000-c8cf-11e9-9f75-2ed2208f955a.png)\n",
    "\n",
    "이 수식을 실제로 계산하면 각 오차를 제곱하여 더하면 되므로 *4 + 100 + 49 + 25 = 178*이 됩니다.\n",
    "\n",
    "이때 데이터의 ***개수인 n으로 나누면, 오차의 제곱합에 대한 평균을 구할 수 있는데 이를 평균 제곱 오차(Mean Squered Error, MSE)***라고 합니다. 수식은 아래와 같습니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/63741373-cc20d500-c8cf-11e9-8462-3315dd01be64.png)\n",
    "\n",
    "이를 실제로 계산하면 178을 4로 나눈 값인 44.5가 됩니다. 이는 y=13x+1의 예측값과 실제값의 평균 제곱 오차의 값이 44.5임을 의미합니다. 평균 제곱 오차는 이번 회귀 문제에서 적절한 W와 b를 찾기위해서 최적화된 식입니다. 그 이유는 평균 제곱 오차의 값을 최소값으로 만드는 W와 b를 찾아내는 것이 정답인 직선을 찾아내는 일이기 때문입니다. 평균 제곱 오차를 W와 b에 의한 비용 함수(Cost function)로 재정의해보면 다음과 같습니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/63741621-c7a8ec00-c8d0-11e9-9e6f-9315332155ad.png)\n",
    "\n",
    "모든 점들과의 오차가 클 수록 평균 제곱 오차는 커지며, 오차가 작아질 수록 평균 제곱 오차는 작아집니다. 그러므로 이 평균 최곱 오차. 즉, Cost(W,b)를 최소가 되게 만드는 W와 b를 구하면 결과적으로 y와 x의 관계를 가장 잘 나타내는 직선을 그릴 수 있게 됩니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/63741470-30439900-c8d0-11e9-88a7-a9f753109ff5.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. 옵티마이저(Optimizer) : 경사하강법(Gradient Descent)**\n",
    "\n",
    "이제 앞서 정의한 비용 함수(Cost Function)의 값을 최소로 하는 W와 b를 찾는 방법에 대해서 배울 차례입니다. 선형 회귀를 포함한 수많은 머신 러닝, 딥 러닝의 학습은 결국 비용 함수를 최소화하는 매개 변수인 W와 b을 찾기 위한 작업을 수행합니다. 이때 사용되는 것이 **옵티마이저(Optimizer)** 알고리즘입니다. **최적화 알고리즘**이라고도 부릅니다. 그리고 이 옵티마이저 알고리즘을 통해 적절한 W와 b를 찾아내는 과정을 머신 러닝에서 학습(training)이라고 부릅니다. 여기서는 가장 기본적인 옵티마이저 알고리즘인 경사 하강법(Gradient Descent)에 대해서 배웁니다.\n",
    "\n",
    "경사 하강법을 이해하기 위해서 cost와 기울기 W와의 관계를 이해해보겠습니다. W는 머신 러닝 용어로는 가중치라고 불리지만, 직선의 방정식 관점에서 보면 직선의 기울기를 의미하고 있습니다. 아래의 그래프는 기울기 W가 지나치게 높거나, 낮을 때 어떻게 오차가 커지는 보여줍니다.\n",
    "\n",
    "![img](https://wikidocs.net/images/page/21670/%EC%86%8C%EB%AC%B8%EC%9E%90x%EB%8B%A4%EC%8B%9C.PNG)\n",
    "\n",
    "위의 그림에서 노란색선은 기울기 W가 20일 때, 초록색선은 기울기 W가 1일 때를 보여줍니다. 다시 말하면 각각\n",
    "\n",
    "- *y = 20x*\n",
    "- *y = x*\n",
    "\n",
    "위에 해당되는 직선입니다. ↕는 각 점에서의 실제값과 두 직선의 예측값과의 오차를 보여줍니다. 이는 앞서 예측에 사용했던 y = 13x + 1 직선보다 확연히 큰 오차값들입니다. 즉, 기울기가 지나치게 크면 실제값과 예측값의 오차가 커지고, 기울기가 지나치게 작아도 실제값과 예측값의 오차가 커집니다. 사실 b 또한 마찬가지인데 b가 지나치게 크거나 작으면 오차가 커집니다.\n",
    "\n",
    "설명의 편의를 위해 편향 b가 없이 단순히 가중치 W만을 사용한 y=Wx라는 가설 H(x)를 가지고, 경사 하강법을 수행한다고 해보겠습니다. 비용 함수의 값 cost(W)는 cost라고 줄여서 표현해보겠습니다. 이에 따라 W와 cost의 관계를 그래프로 표현하면 다음과 같습니다.\n",
    "\n",
    "![img](https://wikidocs.net/images/page/21670/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EC%99%80%EC%BD%94%EC%8A%A4%ED%8A%B8.PNG)\n",
    "\n",
    "기울기 W가 무한대로 커지면 커질 수록 cost의 값 또한 무한대로 커지고, 반대로 기울기 W가 무한대로 작아져도 cost의 값은 무한대로 커집니다. ***위의 그래프에서 cost가 가장 작을 때는 맨 아래의 볼록한 부분***입니다. 기계가 해야할 일은 cost가 가장 최소값을 가지게 하는 W를 찾는 일이므로, 맨 아래의 ***볼록한 부분의 W의 값을 찾아야 합니다.***\n",
    "\n",
    "![img](https://wikidocs.net/images/page/21670/%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95.PNG)\n",
    "\n",
    "기계는 임의의 랜덤값 W값을 정한 뒤에, 맨 아래의 볼록한 부분을 향해 점차 W의 값을 수정해나갑니다. 위의 그림은 W값이 점차 수정되는 과정을 보여줍니다. 그리고 이를 가능하게 하는 것이 경사 하강법(Gradient Descent)입니다. 이를 이해하기 위해서는 고등학교 수학 과정인 미분을 이해해야 합니다. 경사 하강법은 미분을 배우게 되면 가장 처음 배우게 되는 개념인 한 점에서의 순간 변화율 또는 다른 표현으로는 접선에서의 기울기의 개념을 사용합니다.\n",
    "\n",
    "![img](https://wikidocs.net/images/page/21670/%EC%A0%91%EC%84%A0%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B01.PNG)\n",
    "\n",
    "위의 그림에서 초록색 선은 W가 임의의 값을 가지게 되는 네 가지의 경우에 대해서, 그래프 상으로 접선의 기울기를 보여줍니다. 주목할 것은 맨 아래의 볼록한 부분으로 갈수록 접선의 기울기가 점차 작아진다는 점입니다. 그리고 맨 아래의 볼록한 부분에서는 결국 접선의 기울기가 0이 됩니다. 그래프 상으로는 초록색 화살표가 수평이 되는 지점입니다.\n",
    "\n",
    "즉, cost가 최소화가 되는 지점은 접선의 기울기가 0이 되는 지점이며, 또한 미분값이 0이 되는 지점입니다. 경사 하강법의 아이디어는 비용 함수(Cost function)를 미분하여 현재 W에서의 접선의 기울기를 구하고, 접선의 기울기가 낮은 방향으로 W의 값을 변경하고 다시 미분하고 이 과정을 접선의 기울기가 0인 곳을 향해 W의 값을 변경하는 작업을 반복하는 것에 있습니다.\n",
    "\n",
    "수식을 통해 이해해보도록 하겠습니다. 우선, 기존의 비용 함수(Cost function)는 다음과 같습니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/63741916-ee1b5700-c8d1-11e9-8b5c-93347c2df40b.png)\n",
    "\n",
    "이제 cost를 최소화하는 W를 구하기 위한 식은 다음과 같습니다. 해당 식은 접선의 기울기가 0이 될 때까지 반복합니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/63741931-fd020980-c8d1-11e9-8623-fe48a4a03c35.png)\n",
    "\n",
    "위의 식은 현재 W에서의 접선의 기울기와 α와 곱한 값을 현재 W에서 빼서 새로운 W의 값으로 한다는 것을 의미합니다. α는 여기서 학습률(learning rate)이라고 하는데, 우선 α는 생각하지 않고 현재 W에서 현재 W에서의 접선의 기울기를 빼는 행위가 어떤 의미가 있는지 알아보겠습니다.\n",
    "\n",
    "![img](https://wikidocs.net/images/page/21670/%EB%AF%B8%EB%B6%84.PNG)\n",
    "\n",
    "위의 그림은 접선의 기울기가 음수일 때, 0일때, 양수일 때의 경우를 보여줍니다. 접선의 기울기가 음수일 때는 위의 수식이 아래와 같이 표현할 수 있습니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/63742109-b82aa280-c8d2-11e9-924a-01415571d587.png)\n",
    "\n",
    "즉, 기울기가 음수면 W의 값이 증가하게 되는데 이는 결과적으로 접선의 기울기가 0인 방향으로 W의 값이 조정됩니다. 만약, 접선의 기울기가 양수라면 위의 수식은 아래와 같이 표현할 수 있습니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/63742141-cd073600-c8d2-11e9-8c8a-2d869a27281c.png)\n",
    "\n",
    "기울기가 양수면 W의 값이 감소하게 되는데 이는 결과적으로 기울기가 0인 방향으로 W의 값이 조정됩니다. 즉, 아래의 수식은 접선의 기울기가 음수거나, 양수일 때 모두 접선의 기울기가 0인 방향으로 W의 값을 조정합니다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/42408554/63742157-ddb7ac00-c8d2-11e9-9559-f79f28278497.png)\n",
    "\n",
    "그렇다면 여기서 학습률(learning rate)이라고 말하는 α는 어떤 의미를 가질까요? 학습률 α은 W의 값을 변경할 때, 얼마나 크게 변경할지를 결정합니다. 또는 W를 그래프의 한 점으로보고 접선의 기울기가 0일 때까지 경사를 따라 내려간다는 관점에서는 얼마나 큰 폭으로 이동할지를 결정합니다. 직관적으로 생각하기에 학습률 α의 값을 무작정 크게 하면 접선의 기울기가 최소값이 되는 W를 빠르게 찾을 수 있을 것같지만 그렇지 않습니다.\n",
    "\n",
    "![img](https://wikidocs.net/images/page/21670/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EB%B0%9C%EC%82%B0.PNG)\n",
    "\n",
    "위의 그림은 학습률 α가 지나치게 높은 값을 가질 때, 접선의 기울기가 0이 되는 W를 찾아가는 것이 아니라 W의 값이 발산하는 상황을 보여줍니다. 반대로 학습률 α가 지나치게 낮은 값을 가지면 학습 속도가 느려지므로 적당한 α의 값을 찾아내는 것도 중요합니다. 지금까지는 b는 배제시키고 최적의 W를 찾아내는 것에만 초점을 맞추어 경사 하강법의 원리에 대해서 배웠는데, 실제 경사 하강법은 W와 b에 대해서 동시에 경사 하강법을 수행하면서 최적의 W와 b의 값을 찾아갑니다.\n",
    "\n",
    "정리하자면 가설, 비용 함수, 옵티마이저는 머신 러닝 분야에서 사용되는 포괄적 개념입니다. 풀고자하는 각 문제에 따라 가설, 비용 함수, 옵티마이저는 전부 다를 수 있으며 선형 회귀에 가장 적합한 비용 함수와 옵티마이저가 알려져있는데 이번 챕터에서 언급된 MSE와 경사 하강법이 각각 이에 해당됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. 케라스로 구현하는 선형 회귀**\n",
    "\n",
    "케라스에 대해서는 뒤의 딥 러닝 챕터에서 더 자세히 배우겠지만, 우선 간단하게 케라스를 이용해서 선형 회귀를 구현해보도록 하겠습니다. 우선 케라스로 모델을 만드는 가장 기본적인 형식은 다음과 같습니다. 아래의 코드는 아직 완전한 코드가 아니므로 실행이 불가합니다. 단지 설명을 위해 앞으로 사용하게 될 코드를 미리 일부 가져왔을 뿐입니다.\n",
    "\n",
    "```python\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(1,input_dim=1))\n",
    "```\n",
    "\n",
    "Sequential로 model이라는 이름의 모델을 만들고, 그리고 add를 통해 필요한 사항들을 추가해갑니다. 첫번째 인자인 1은 출력의 차원을 의미하며, 두번째 인자인 input_dim은 입력의 차원을 정의하는데 이번 실습과 같이 1개의 실수 x를 가지고 하는 1개의 실수 y를 예측하는 단순 선형 회귀를 구현하는 경우에는 각각 1의 값을 가집니다. 이제 직접 실습을 진행해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\young\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\young\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/300\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 373.5317 - mean_squared_error: 373.5317\n",
      "Epoch 2/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2824 - mean_squared_error: 2.2824\n",
      "Epoch 3/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2774 - mean_squared_error: 2.2774\n",
      "Epoch 4/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2727 - mean_squared_error: 2.2727\n",
      "Epoch 5/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2681 - mean_squared_error: 2.2681\n",
      "Epoch 6/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2637 - mean_squared_error: 2.2637\n",
      "Epoch 7/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2594 - mean_squared_error: 2.2594\n",
      "Epoch 8/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.2553 - mean_squared_error: 2.2553\n",
      "Epoch 9/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2514 - mean_squared_error: 2.2514\n",
      "Epoch 10/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2477 - mean_squared_error: 2.2477\n",
      "Epoch 11/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2440 - mean_squared_error: 2.2440\n",
      "Epoch 12/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2405 - mean_squared_error: 2.2405\n",
      "Epoch 13/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2372 - mean_squared_error: 2.2372\n",
      "Epoch 14/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.2340 - mean_squared_error: 2.2340\n",
      "Epoch 15/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2308 - mean_squared_error: 2.2308\n",
      "Epoch 16/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2278 - mean_squared_error: 2.2278\n",
      "Epoch 17/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2250 - mean_squared_error: 2.2250\n",
      "Epoch 18/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.2222 - mean_squared_error: 2.2222\n",
      "Epoch 19/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2195 - mean_squared_error: 2.2195\n",
      "Epoch 20/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2169 - mean_squared_error: 2.2169\n",
      "Epoch 21/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2145 - mean_squared_error: 2.2145\n",
      "Epoch 22/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2121 - mean_squared_error: 2.2121\n",
      "Epoch 23/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.2098 - mean_squared_error: 2.2098\n",
      "Epoch 24/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2076 - mean_squared_error: 2.2076\n",
      "Epoch 25/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2054 - mean_squared_error: 2.2054\n",
      "Epoch 26/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.2034 - mean_squared_error: 2.2034\n",
      "Epoch 27/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.2014 - mean_squared_error: 2.2014\n",
      "Epoch 28/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.1995 - mean_squared_error: 2.1995\n",
      "Epoch 29/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1976 - mean_squared_error: 2.1976\n",
      "Epoch 30/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1959 - mean_squared_error: 2.1959\n",
      "Epoch 31/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1941 - mean_squared_error: 2.1941\n",
      "Epoch 32/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1925 - mean_squared_error: 2.1925\n",
      "Epoch 33/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1909 - mean_squared_error: 2.1909\n",
      "Epoch 34/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1894 - mean_squared_error: 2.1894\n",
      "Epoch 35/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.1879 - mean_squared_error: 2.1879\n",
      "Epoch 36/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1864 - mean_squared_error: 2.1864\n",
      "Epoch 37/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1850 - mean_squared_error: 2.1850\n",
      "Epoch 38/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1837 - mean_squared_error: 2.1837\n",
      "Epoch 39/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.1824 - mean_squared_error: 2.1824\n",
      "Epoch 40/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1812 - mean_squared_error: 2.1812\n",
      "Epoch 41/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1800 - mean_squared_error: 2.1800\n",
      "Epoch 42/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1788 - mean_squared_error: 2.1788\n",
      "Epoch 43/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1777 - mean_squared_error: 2.1777\n",
      "Epoch 44/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1767 - mean_squared_error: 2.1767\n",
      "Epoch 45/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1756 - mean_squared_error: 2.1756\n",
      "Epoch 46/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1746 - mean_squared_error: 2.1746\n",
      "Epoch 47/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1737 - mean_squared_error: 2.1737\n",
      "Epoch 48/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.1727 - mean_squared_error: 2.1727\n",
      "Epoch 49/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1718 - mean_squared_error: 2.1718\n",
      "Epoch 50/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1709 - mean_squared_error: 2.1709\n",
      "Epoch 51/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1701 - mean_squared_error: 2.1701\n",
      "Epoch 52/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.1693 - mean_squared_error: 2.1693\n",
      "Epoch 53/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1685 - mean_squared_error: 2.1685\n",
      "Epoch 54/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1678 - mean_squared_error: 2.1678\n",
      "Epoch 55/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1670 - mean_squared_error: 2.1670\n",
      "Epoch 56/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1663 - mean_squared_error: 2.1663\n",
      "Epoch 57/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1656 - mean_squared_error: 2.1656\n",
      "Epoch 58/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.1650 - mean_squared_error: 2.1650\n",
      "Epoch 59/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1643 - mean_squared_error: 2.1643\n",
      "Epoch 60/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1637 - mean_squared_error: 2.1637\n",
      "Epoch 61/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1631 - mean_squared_error: 2.1631\n",
      "Epoch 62/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1626 - mean_squared_error: 2.1626\n",
      "Epoch 63/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1620 - mean_squared_error: 2.1620\n",
      "Epoch 64/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1615 - mean_squared_error: 2.1615\n",
      "Epoch 65/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1610 - mean_squared_error: 2.1610\n",
      "Epoch 66/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1605 - mean_squared_error: 2.1605\n",
      "Epoch 67/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1600 - mean_squared_error: 2.1600\n",
      "Epoch 68/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1595 - mean_squared_error: 2.1595\n",
      "Epoch 69/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1591 - mean_squared_error: 2.1591\n",
      "Epoch 70/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 2.6041 - mean_squared_error: 2.60 - 0s 1ms/step - loss: 2.1586 - mean_squared_error: 2.1586\n",
      "Epoch 71/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 997us/step - loss: 2.1582 - mean_squared_error: 2.1582\n",
      "Epoch 72/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1578 - mean_squared_error: 2.1578\n",
      "Epoch 73/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1574 - mean_squared_error: 2.1574\n",
      "Epoch 74/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.1570 - mean_squared_error: 2.1570\n",
      "Epoch 75/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1567 - mean_squared_error: 2.1567\n",
      "Epoch 76/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1563 - mean_squared_error: 2.1563\n",
      "Epoch 77/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1560 - mean_squared_error: 2.1560\n",
      "Epoch 78/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1556 - mean_squared_error: 2.1556\n",
      "Epoch 79/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1553 - mean_squared_error: 2.1553\n",
      "Epoch 80/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1550 - mean_squared_error: 2.1550\n",
      "Epoch 81/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1547 - mean_squared_error: 2.1547\n",
      "Epoch 82/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1544 - mean_squared_error: 2.1544\n",
      "Epoch 83/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1541 - mean_squared_error: 2.1541\n",
      "Epoch 84/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1539 - mean_squared_error: 2.1539\n",
      "Epoch 85/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1536 - mean_squared_error: 2.1536\n",
      "Epoch 86/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1534 - mean_squared_error: 2.1534\n",
      "Epoch 87/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1531 - mean_squared_error: 2.1531\n",
      "Epoch 88/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1529 - mean_squared_error: 2.1529\n",
      "Epoch 89/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1526 - mean_squared_error: 2.1526\n",
      "Epoch 90/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1524 - mean_squared_error: 2.1524\n",
      "Epoch 91/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1522 - mean_squared_error: 2.1522\n",
      "Epoch 92/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1520 - mean_squared_error: 2.1520\n",
      "Epoch 93/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1518 - mean_squared_error: 2.1518\n",
      "Epoch 94/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1516 - mean_squared_error: 2.1516\n",
      "Epoch 95/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1514 - mean_squared_error: 2.1514\n",
      "Epoch 96/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1513 - mean_squared_error: 2.1513\n",
      "Epoch 97/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1511 - mean_squared_error: 2.1511\n",
      "Epoch 98/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1509 - mean_squared_error: 2.1509\n",
      "Epoch 99/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1508 - mean_squared_error: 2.1508\n",
      "Epoch 100/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1506 - mean_squared_error: 2.1506\n",
      "Epoch 101/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1504 - mean_squared_error: 2.1504\n",
      "Epoch 102/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1503 - mean_squared_error: 2.1503\n",
      "Epoch 103/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1502 - mean_squared_error: 2.1502\n",
      "Epoch 104/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1500 - mean_squared_error: 2.1500\n",
      "Epoch 105/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1499 - mean_squared_error: 2.1499\n",
      "Epoch 106/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1498 - mean_squared_error: 2.1498\n",
      "Epoch 107/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1496 - mean_squared_error: 2.1496\n",
      "Epoch 108/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.1495 - mean_squared_error: 2.1495\n",
      "Epoch 109/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1494 - mean_squared_error: 2.1494\n",
      "Epoch 110/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1493 - mean_squared_error: 2.1493\n",
      "Epoch 111/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1492 - mean_squared_error: 2.1492\n",
      "Epoch 112/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1491 - mean_squared_error: 2.1491\n",
      "Epoch 113/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1490 - mean_squared_error: 2.1490\n",
      "Epoch 114/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1489 - mean_squared_error: 2.1489\n",
      "Epoch 115/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1488 - mean_squared_error: 2.1488\n",
      "Epoch 116/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1487 - mean_squared_error: 2.1487\n",
      "Epoch 117/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1486 - mean_squared_error: 2.1486\n",
      "Epoch 118/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1485 - mean_squared_error: 2.1485\n",
      "Epoch 119/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1484 - mean_squared_error: 2.1484\n",
      "Epoch 120/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1484 - mean_squared_error: 2.1484\n",
      "Epoch 121/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1483 - mean_squared_error: 2.1483\n",
      "Epoch 122/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1482 - mean_squared_error: 2.1482\n",
      "Epoch 123/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1481 - mean_squared_error: 2.1481\n",
      "Epoch 124/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1481 - mean_squared_error: 2.1481\n",
      "Epoch 125/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1480 - mean_squared_error: 2.1480\n",
      "Epoch 126/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1479 - mean_squared_error: 2.1479\n",
      "Epoch 127/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1479 - mean_squared_error: 2.1479\n",
      "Epoch 128/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1478 - mean_squared_error: 2.1478\n",
      "Epoch 129/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1478 - mean_squared_error: 2.1478\n",
      "Epoch 130/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1477 - mean_squared_error: 2.1477\n",
      "Epoch 131/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1476 - mean_squared_error: 2.1476\n",
      "Epoch 132/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1476 - mean_squared_error: 2.1476\n",
      "Epoch 133/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1475 - mean_squared_error: 2.1475\n",
      "Epoch 134/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1475 - mean_squared_error: 2.1475\n",
      "Epoch 135/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1474 - mean_squared_error: 2.1474\n",
      "Epoch 136/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1474 - mean_squared_error: 2.1474\n",
      "Epoch 137/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1473 - mean_squared_error: 2.1473\n",
      "Epoch 138/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1473 - mean_squared_error: 2.1473\n",
      "Epoch 139/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1473 - mean_squared_error: 2.1473\n",
      "Epoch 140/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1472 - mean_squared_error: 2.1472\n",
      "Epoch 141/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1472 - mean_squared_error: 2.1472\n",
      "Epoch 142/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1471 - mean_squared_error: 2.1471\n",
      "Epoch 143/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1471 - mean_squared_error: 2.1471\n",
      "Epoch 144/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1471 - mean_squared_error: 2.1471\n",
      "Epoch 145/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1470 - mean_squared_error: 2.1470\n",
      "Epoch 146/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 997us/step - loss: 2.1470 - mean_squared_error: 2.1470\n",
      "Epoch 147/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1470 - mean_squared_error: 2.1470\n",
      "Epoch 148/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1469 - mean_squared_error: 2.1469\n",
      "Epoch 149/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1469 - mean_squared_error: 2.1469\n",
      "Epoch 150/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1469 - mean_squared_error: 2.1469\n",
      "Epoch 151/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1468 - mean_squared_error: 2.1468\n",
      "Epoch 152/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1468 - mean_squared_error: 2.1468\n",
      "Epoch 153/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1468 - mean_squared_error: 2.1468\n",
      "Epoch 154/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1468 - mean_squared_error: 2.1468\n",
      "Epoch 155/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1468 - mean_squared_error: 2.1468\n",
      "Epoch 156/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1467 - mean_squared_error: 2.1467\n",
      "Epoch 157/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1467 - mean_squared_error: 2.1467\n",
      "Epoch 158/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1467 - mean_squared_error: 2.1467\n",
      "Epoch 159/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1467 - mean_squared_error: 2.1467\n",
      "Epoch 160/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1466 - mean_squared_error: 2.1466\n",
      "Epoch 161/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1466 - mean_squared_error: 2.1466\n",
      "Epoch 162/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1466 - mean_squared_error: 2.1466\n",
      "Epoch 163/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1466 - mean_squared_error: 2.1466\n",
      "Epoch 164/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1466 - mean_squared_error: 2.1466\n",
      "Epoch 165/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1465 - mean_squared_error: 2.1465\n",
      "Epoch 166/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1465 - mean_squared_error: 2.1465\n",
      "Epoch 167/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1465 - mean_squared_error: 2.1465\n",
      "Epoch 168/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1465 - mean_squared_error: 2.1465\n",
      "Epoch 169/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1465 - mean_squared_error: 2.1465\n",
      "Epoch 170/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1465 - mean_squared_error: 2.1465\n",
      "Epoch 171/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1464 - mean_squared_error: 2.1464\n",
      "Epoch 172/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1464 - mean_squared_error: 2.1464\n",
      "Epoch 173/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1464 - mean_squared_error: 2.1464\n",
      "Epoch 174/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1464 - mean_squared_error: 2.1464\n",
      "Epoch 175/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1464 - mean_squared_error: 2.1464\n",
      "Epoch 176/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1464 - mean_squared_error: 2.1464\n",
      "Epoch 177/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1464 - mean_squared_error: 2.1464\n",
      "Epoch 178/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1464 - mean_squared_error: 2.1464\n",
      "Epoch 179/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1463 - mean_squared_error: 2.1463\n",
      "Epoch 180/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1463 - mean_squared_error: 2.1463\n",
      "Epoch 181/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1463 - mean_squared_error: 2.1463\n",
      "Epoch 182/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1463 - mean_squared_error: 2.1463\n",
      "Epoch 183/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1463 - mean_squared_error: 2.1463\n",
      "Epoch 184/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1463 - mean_squared_error: 2.1463\n",
      "Epoch 185/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1463 - mean_squared_error: 2.1463\n",
      "Epoch 186/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1463 - mean_squared_error: 2.1463\n",
      "Epoch 187/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1463 - mean_squared_error: 2.1463\n",
      "Epoch 188/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1463 - mean_squared_error: 2.1463\n",
      "Epoch 189/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 190/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 191/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 192/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 193/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 194/300\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 195/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 196/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 197/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 198/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 199/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 200/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 201/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 202/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 203/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 204/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 205/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 206/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1462 - mean_squared_error: 2.1462\n",
      "Epoch 207/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 208/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 209/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 210/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 211/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 212/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 213/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 214/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 215/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 216/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 217/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 218/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 219/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 220/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 221/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 222/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 223/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 224/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 225/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 226/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 227/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 228/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 229/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 230/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 231/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 232/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 233/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 234/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 235/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 236/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 237/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 238/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 239/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 240/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 241/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 242/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 243/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 244/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 245/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 246/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 247/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 248/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 249/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 250/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 251/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 252/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 253/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 254/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1461 - mean_squared_error: 2.1461\n",
      "Epoch 255/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 256/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 257/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 258/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 259/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 260/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 261/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 262/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 263/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 264/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 265/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 266/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 267/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 268/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 269/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 270/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 271/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 272/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 273/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 274/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 275/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 276/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 277/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 278/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 279/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 280/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 281/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 282/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 283/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 284/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 285/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 286/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 287/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 288/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 289/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 290/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 291/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 292/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 293/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 294/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 295/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1460 - mean_squared_error: 2.1460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 297/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 298/300\n",
      "9/9 [==============================] - 0s 887us/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 299/300\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1460 - mean_squared_error: 2.1460\n",
      "Epoch 300/300\n",
      "9/9 [==============================] - 0s 997us/step - loss: 2.1460 - mean_squared_error: 2.1460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fd689bc550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential # 케라스의 Sequential()을 임포트\n",
    "from keras.layers import Dense # 케라스의 Dense()를 임포트\n",
    "from keras import optimizers # 케라스의 옵티마이저를 임포트\n",
    "import numpy as np # Numpy를 임포트\n",
    "\n",
    "X = np.array([1,2,3,4,5,6,7,8,9]) # 공부하는 시간\n",
    "y = np.array([11,22,33,44,53,66,77,87,95]) # 각 공부하는 시간에 맵핑되는 성적\n",
    "\n",
    "model = Sequential()\n",
    "# activation은 어떤 함수를 사용할 것인지를 의미하는데 선형 회귀를 사용할 경우에는 linear라고 기재\n",
    "model.add(Dense(1, input_dim = 1, activation = 'linear'))\n",
    "# 학습률(learning rate, lr)은 0.01로 합니다.\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "\n",
    "# 옵티마이저는 경사하강법의 일종인 확률적 경사 하강법 sgd를 사용합니다.\n",
    "# 손실 함수(Loss function)은 평균제곱오차 mse를 사용합니다.\n",
    "model.compile(optimizer = sgd, loss = 'mse', metrics = ['mse'])\n",
    "\n",
    "# 주어진 X와 y데이터에 대해서 오차를 최소화하는 작업을 300번 시도합니다.\n",
    "model.fit(X, y, batch_size = 1, epochs = 300, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드는 간단하지만, 지금까지 배운 것들이 집대성 된 코드입니다. 우선 공부한 시간을 x, 각 공부한 시간에 따른 성적을 y라고 해봅시다. activation은 어떤 함수를 사용할 것인지를 의미하는데 선형 회귀를 사용할 경우에는 linear라고 기재합니다.\n",
    "\n",
    "옵티마이저로는 경사 하강법의 일종인 확률적 경사 하강법을 사용하였으며, 학습률은 0.01로 정하였습니다. 손실 함수로는 평균 제곱 오차를 사용합니다. 그리고 전체 데이터에 대한 훈련 횟수는 300으로 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 데이터에 대한 훈련 횟수는 300으로 하였지만, 어느 순간 오차가 더 이상 줄어들지 않는데 이는 오차를 최소화하는 가중치 W와 b를 찾았기 때문으로 추정이 가능합니다. 이제 최종적으로 선택된 오차를 최소화하는 직선을 그래프로 그려보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fd5e7daf98>,\n",
       " <matplotlib.lines.Line2D at 0x1fd5e7de128>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X, model.predict(X), 'b', X,y, 'k.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그래프에서 각 점은 우리가 실제 주었던 실제값에 해당되며, 직선은 실제값으로부터 오차를 최소화하는 W와 b의 값을 가지는 직선입니다. 이제 이 직선을 통해 9시간 30분을 공부하였을 때의 시험 성적을 예측하게 해봅시다. model.predict()은 학습이 완료된 모델이 입력된 데이터에 대해서 어떤 값을 예측하는지를 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[98.556465]]\n"
     ]
    }
   ],
   "source": [
    "# 9시간 30분을 공부하면 약 98.5점을 얻는다고 예측하고 있습니다.\n",
    "print(model.predict([9.5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
