{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **텐서플로**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **tf.keras.layer.Dense**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **아래는 입력 값에 대해 활성화 함수로 시그모이드 함수를 사용하고, 출력 값으로 10개의 값을 출력하는 완전 연결 계층(Fully Connected Layer) 정의한 것이다.**<br>\n",
    "    * 시그모이드 함수 참고 사이트: https://icim.nims.re.kr/post/easyMath/64\n",
    "\n",
    "* **완전 연결 계층(Fully Connected Layer)란? https://kolikim.tistory.com/53**\n",
    "    * 완전 연결 신경망이란, 인접하는 계층의 모든 뉴런과 결합되어 있는 신경망을 말한다. 입력 데이터와 가중치가 1대 1로 대응하는 것을 완전 연결(Fully-Connected)이라고 하며, 행렬의 내적(Affine)으로 표현된다. 이 완전 계층의 문제점은 바로 데이터의 형상이 무시된다는 것이다. 입력 데이터가 다차원의 형상일 때, 완전 연결 계층에 입력해주기 위해서는 이 다차원 데이터를 1차원으로 평탄화해준 후에 입력하여야만 한다.\n",
    "    * 가장 흔한 다차원 데이터인 이미지를 생각해보면, 이미지 하나는 2차원의 픽셀 배열에 RGB의 채널이 합쳐져 3차원을 이루고 있다. 이 RGB의 각 채널은 서로 밀접하게 관련되어 있고, 픽셀 간의 거리에 따라 또한 관련되어 있는 등 3차원 속에서 의미를 갖는 패턴이 숨어 있다. 그러나 완전 연결 계층은 형상을 무시하고 모든 입력 데이터를 동일 차원으로 취급하기에 형상에 담긴 정보를 살릴 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Dense 함수 options**\n",
    "    * **y =f(Wx + b)**\n",
    "        1. units: 출력 값의 크기, Integer 혹은 Long 형태\n",
    "        2. ativation: 활성화 함수\n",
    "        3. use_bias: 편향(b)을 사용할지 여부, Boolean 값 형태\n",
    "        4. kernel_initializer: 가중치(W) 초기화 함수\n",
    "        5. bias_initializer: 편향 초기화 함수\n",
    "        6. kernel_regularizer: 가중치 정규화 방법\n",
    "        7. bias_regularizer: 편향 정규화 방법\n",
    "        8. kernel_constraint: Optimizer에 의해 업데이트 된 이후에 가중치에 적용되는 부가적인 제약 함수 (예시: norm constraint, value constraint)\n",
    "        9. bias_constraint: Optimizer에 의해 업데이트 된 이후에 편향에 적용되는 부가적인 제약 함수 (예시: norm contraint, value constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (20, 1)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)\n",
    "output = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **아래는 10개의 노드를 가지는 은닉층이 있고 최종 출력 값은 2개의 노드가 있는 신경망 구조를 정의한 것이다. 객체를 두 개 생성하여 신경망을 만들 수 있다.**\n",
    "\n",
    "* **은닉층**\n",
    "참고사이트: https://blog.lgcns.com/1359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (20, 1)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)\n",
    "hidden = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(input)\n",
    "output = tf.keras.layers.Dense(units = 2, activation = tf.nn.sigmoid)(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **tf.keras.layers.Dropout**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 신경망 모델을 만들 때 생기는 여러 문제점 중 대표적인 문제점은 **과적합(Overfitting)** 이다. 과적합 문제는 **정규화(Regularization)** 방법을 사용하여 해결하는데, 그중 가장 대표적인 방법이 **드롭아웃(Dropout)** 이다.\n",
    "\n",
    "* 텐서플로는 드롭아웃을 쉽게 모델에 적용할 수 있게 간단한 모듈을 제공하는데, 이 모듈을 이용하면 특정 keras.layers의 입력 값에 드롭아웃을 적용할 수 있다. 사용법은 위의 dense 층을 만드는 방법과 유사하게 Dropout 객체를 생성해서 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Drop 함수 options**\n",
    "    1. rate: 드롭아웃을 적용할 확률을 지정한다. 확률 값이므로 0 ~ 1 사이의 값을 받는다. 예를 들어 dropout = 0.2로 지정하면 전체 입력 값 중에서 20%를 0으로 만든다.\n",
    "    2. noise_shape: 정수형의 1D-tensor 값을 받는다. 여기서 받은 값은 shape을 뜻하는데, 이 값을 지정함으로써 특정 값만 드롭아웃을 적용할 수 있다. 예를 들면, 입력값이 이미지일 때 noise_shape을 지정하면 특정 채널에만 드롭아웃을 적용할 수 있다.\n",
    "    3. seed: 드롭아웃의 경우 지정된 확률 값을 바탕으로 무작위로 드롭아웃을 적용하는데, 이때 임의의 선택을 위한 시드값을 의미한다. seed 값은 정수형이며, 같은 seed 값을 가지는 드롭아웃의 경우 동일한 드롭아웃 결과를 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 드롭아웃은 학습 데이터에 과적합되는 상황을 방지하기 위해 학습 시 특정 황률로 노드들의 값을 0으로 만든다. 그리고 이러한 과정은 학습할 때만 적용되고 예측 혹은 테스트할 때는 적용되지 않아야 한다. 케라스의 드롭아웃을 사용할 경우 이러한 부분이 자동으로 적용된다. 드롭아웃을 적용하는 방법은 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (20, 1)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.5)(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 텐서플로에서 드롭아웃은 tf.keras.layers뿐만 아니라 tf.nn 모듈에도 있는데, 두 모듈의 차이점은 tf.keras.layers.dropout의 경우 확률을 0.2로 지정했을 때 노드의 20%를 0으로 만드는 데 비해 tf.nn.dropout의 경우 확률을 0.2로 지정했을 때 80%를 0으로 만든다는 차이점이 있다.\n",
    "\n",
    "* 드롭아웃을 적용하려는 층의 노드르 객체에 적용하면 된다. 아래 코드의 경우 입력 값에 드롭아웃을 적용한 후 Dense 층을 지나도록 작성했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (20, 1)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.2)(input)\n",
    "hidden = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(dropout)\n",
    "output = tf.keras.layers.Dense(units = 2, activation = tf.nn.sigmoid)(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **tf.keras.layers.Conv1D**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **합성곱이란? http://ppaktion.maru.net/702**\n",
    "    * 콘볼루션은 하나의 신호로 다른 신호를 번지게 하는 것이다. 연필로 종이에 찍은 점을 지우개로 가볍게 문지르면 번져난다. 연필로 찍은 점과 지우개의 콘볼루션이다. 이런 번짐 효과는 포토샵에서 블러 툴을 쓰는 것과도 같다. 이미지 프로세싱에서 필터링을 할 때 어떤 모양의 함수로 콘볼루션을 하느냐에 따라 다양한 출력을 얻는다. 광학에서 초점 밖의 흐릿한 상 역시 바늘구멍과 물체에서 나온 빛의 콘볼루션이다. \n",
    "    * 시간 시스템에서 입력값과 시스템 응답 함수의 콘볼루션으로 출력 값을 얻는다. 결국 같은 것이지만 이미지에서 가로 축은 공간이고 시간 시스템에서는 시간이라는 것이 다르다. 소리 신호가 방에서 반사되어서 울림이 생기는 것도 원래 신호와 방이라는 시스템 응답의 콘볼루션으로 나타낼 수 있다. 델타 함수와의 콘볼루션은 곱하기가 되기 때문에 시스템의 응답이 델타 함수라면 선형 시스템이라고 할 수 있다.\n",
    "    \n",
    "* 텐서플로의 합성곱 연산은 Conv1D, Conv2D, Conv3D로 나눠지는데, 어떤 차이가 있는지 알아보겠다. 우리가 흔히 알고 있는 기본적인 이미지에 적용하는 합성곱 방식은 Conv2D다. 합성곱은 일반적으로 두 가지 기준으로 구분할 수 있다. 바로 합성곱이 진행되는 방향과 합성곱 겨롸로 나오는 출력값이다. 이 두가지를 기준으로 비교한 결과는 다음과 같다.\n",
    "    * Conv1D: 한 방향(가로) / 1-D Array(vector)\n",
    "    * Conv2D: 두 방향(가로, 세로) / 2-D Array(matrix)\n",
    "    * Conv3D: 세 방향(가로, 세로, 높이) / 3-D Array(tensor)\n",
    "* 그러나 출력값의 경우 실제 합성곱 출력값과 동일하진 않다.배치 크기와 합성곱이 적용되는 필터의 개수도 고려해야 하기 때문에 출력값이 위와 동일하게 나오지 않는 것이다. 위의 경우는 단순히 배치의 경우는 고려하지 않고, 합성곱 필터를 하나만 적용했을 때라고 생각하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **합성곱 함수 options**\n",
    "    * 합성곱도 필터의 크기, 필터의 개수, 스트라이드 값 등으로 객체를 생성할 때 인자로 설정할 수 있다. 구조는 몇 가지를 제외하고 이전에 알아본 Dense와 비슷하다. 다른 점은 합성곱 연산을 수행할 필터와 관련된 부분이다. 그리고 합성곱은 기본적으로 필터의 크기를 필요로 하는데, 이 경우 Conv1D는 필터의 높이(high)는 필요하지 않다. Conv1D의 필터는 입력값의 차원수와 높이가 동일하게 연산되기 때문에 필터의 가로 길이만 설정하면 된다. 즉, 필터의 가로에 적용되는 kernel_size만 설정하면 된다. 그리고 총 몇 개의 필터를 사용할지를 filters 인자를 통해 정해야 한다.\n",
    "    * 예를 들어 (5, 10) 형태의 입려값에 필터의 크기인 kernel_size를 2로 설정하고 필터의 개수를 10으로 지정할 경우 출력값의 형태는 (1, 4, 10)이 된다. 그리고 패딩을 사용해 입력값과 출력값의 가로 크기를 똑같이 만들고 싶다면, padding = \"same\"을 지정하면 입력과 출력의 가로 값이 같아진다.\n",
    "---\n",
    "1. filters: 필터의 개수로서, 정수형으로 지정한다. 출력의 차원수를 나타낸다.\n",
    "2. kernel_size: 필터의 크기로서, 정수 혹은 정수의 리스트, 튜플 형태로 지정한다. 합성곱이 적용되는 윈도우(window)의 길이를 나타낸다. \n",
    "3. strides: 적용할 스트라이드의 값으로서 정수 혹은 정수의 리스트, 튜플 형태로 지정한다. 1이 아닌 값을 지정할 경우 dilation_rate는 1 이외의 값을 지정하지 못한다.\n",
    "4. padding: 패딩 방법을 정한다. \"VALID\" 또는 \"SAME\"을 지정할 수 있다.\n",
    "5. data_format: 데이터의 표현 방법을 선택한다. \"channel_last\" 혹은 \"channel_first\"를 지정할 수 있다. last의 경우 데이터는 (batch, length, channels) 형태여야 하고, channel_first의 경우 데이터는 (batch, channels, length) 형태여야 한다.\n",
    "6. dilation_rate: dilation 합성곱 사용 시 적용할 dilation 값으로 정수 혹은 정수의 리스트, 튜플 형태로 지정한다. 1이 아닌 값을 지정하면 strides 값으로 1 이외의 값을 지정하지 못한다.\n",
    "7. activation: 활성화 함수\n",
    "8. use_bias: 편향을 사용할지 여부, Boolean 값 형태\n",
    "9. kernel_initializer: 가중치 초기화 함수\n",
    "10. bias_initializer: 편향 초기화 함수\n",
    "11. kernel_regularizer: 가중치 정규화 방법\n",
    "12. bias_regularizer: 편향 정규화 방법\n",
    "13. activity_regularizer: 출력값 정규화 방법\n",
    "14. kernel_constraint: Optimizer에 의해 업데이트 된 이후에 가중치에 적용되는 부가적인 제약 함수 (예시: norm constraint, value constraint)\n",
    "15. bias_constraint: Optimizer에 의해 업데이트 된 이후에 편향에 적용되는 부가적인 제약 함수 (예시: norm contraint, value constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv1D의 기본적인 사용법\n",
    "INPUT_SIZE = (1, 28, 28)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "    filters = 10,\n",
    "    kernel_size = 3,\n",
    "    padding = \"same\",\n",
    "    activation = tf.nn.relu)(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드롭아웃을 적용한 Conv1D 사용법\n",
    "INPUT_SIZE = (1, 28, 28)\n",
    "is_training = True\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.2)(input)\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "    filters = 10,\n",
    "    kernel_size = 3,\n",
    "    padding = \"same\",\n",
    "    activation = tf.nn.relu)(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
